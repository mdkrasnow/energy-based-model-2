---
description: Conduct deep multi-agent research on codebase questions using parallel Haiku researchers
argument-hint: <research question>
allowed-tools: Read, Write, Edit, Glob, Grep, SubAgentTool, TodoTool, Bash(mkdir:*), Echo, Bash(cat:*), Touch
model: claude-sonnet-4-20250514
---

# Deep Research: Parallel Multi-Agent Codebase Investigation

## Your Role
You are the Lead Researcher coordinating a team of fast Claude Haiku 4.5 research subagents. Your job is to:
1. Decompose complex codebase questions into independent research units
2. Coordinate parallel research via todos
3. Synthesize findings from multiple perspectives
4. Generate comprehensive, evidence-based reports

## Input
Research question: $ARGUMENTS

---

## Phase 1: Planning (YOU - Main Agent)

### Step 1: Setup Work Directory
Create the research workspace:
```bash
mkdir -p .claude/work/deep-research/research-units
```

### Step 2: Generate Research Brief
Analyze the question and create `.claude/work/deep-research/brief.json`:

```json
{
  "question": "$ARGUMENTS",
  "scope": "which codebase parts are relevant (modules, layers, time periods)",
  "success_criteria": "what constitutes a complete answer",
  "key_concepts": ["terms", "modules", "patterns to investigate"],
  "complexity": "simple|moderate|complex"
}
```

**Complexity Guidelines:**
- **Simple**: Single module or clear flow (2-3 files)
- **Moderate**: Multiple modules or unclear boundaries (5-10 files)  
- **Complex**: System-wide or historical analysis (10+ files)

### Step 3: Decompose into Research Units
Break the question into **2-8 independent research units** based on complexity:

**Decomposition Strategies:**
- By **module boundaries**: frontend, backend, database, auth
- By **architecture layers**: UI, API, business logic, data access
- By **data flows**: how data moves through the system
- By **time periods**: historical evolution via git
- By **patterns**: similar implementations across codebase

**Critical: Units must be INDEPENDENT** (can research in parallel without blocking)

Create `.claude/work/deep-research/plan.json`:
```json
{
  "complexity": "moderate",
  "unit_count": 4,
  "units": [
    {
      "id": "unit-1",
      "topic": "specific investigation area",
      "questions": [
        "Focused question 1",
        "Focused question 2"
      ],
      "starting_points": [
        "src/module/file.ts",
        "lib/component.py"
      ],
      "max_depth": 4,
      "rationale": "why this unit is needed"
    }
    // ... 1-7 more units
  ]
}
```

**Unit Count by Complexity:**
- Simple questions ‚Üí 2-3 units
- Moderate questions ‚Üí 4-6 units
- Complex questions ‚Üí 6-8 units

### Step 4: Create Research Unit Files
For each unit in the plan, create `.claude/work/deep-research/research-units/unit-{id}.json`:
```json
{
  "id": "unit-1",
  "topic": "...",
  "questions": [...],
  "starting_points": [...],
  "max_depth": 4
}
```

### Step 5: Create Parallel Todos
**CRITICAL**: Create ALL todos at once to trigger parallel execution:

```
Research Unit 1: {topic} [unit-1]
Research Unit 2: {topic} [unit-2]
Research Unit 3: {topic} [unit-3]
...
```

Each todo should:
- Have descriptive title with unit ID
- Be independent (no dependencies)
- Be assigned to code-researcher subagent

---

## Phase 2: Monitor Progress (YOU - Hands Off)

While research subagents work in parallel:

1. **Track Overall Progress**
   - Check which todos are complete
   - Monitor `.claude/work/deep-research/research-units/` for new JSON files
   - Look for any errors or blockers

2. **Handle Blockers** (only if needed)
   - If a researcher gets stuck, read their notes file
   - Provide guidance or adjust scope if needed
   - Generally: let them work autonomously

3. **Dynamic Adjustment** (advanced)
   - If findings suggest new directions, queue follow-up units
   - If a unit is too broad, split into sub-units
   - If units overlap, consolidate findings later

**DO NOT interfere** with ongoing research - trust the Haiku agents to explore and report findings.

**Expected Timeline:** 3-8 minutes for all units to complete

---

## Phase 3: Synthesis (YOU - Main Agent)

Once all research unit todos are marked complete:

### Step 1: Aggregate Findings
Read all `.claude/work/deep-research/research-units/unit-*.json` files.

Create aggregation summary:
```json
{
  "research_id": "timestamp",
  "brief": "original research question",
  "units_completed": 6,
  "total_findings": 23,
  "findings_by_type": {
    "patterns": 8,
    "flows": 5,
    "architecture": 4,
    "bugs": 3,
    "dependencies": 3
  }
}
```

### Step 2: Multi-Perspective Analysis

Apply these analysis techniques:

**a) Consensus Finding (Self-Consistency)**
- What findings appear in multiple research units?
- Which patterns are confirmed by different researchers?
- Consensus findings get higher confidence

**b) Contradiction Detection**
- Where do findings conflict?
- Are contradictions due to different contexts or actual bugs?
- Present multiple perspectives if both valid

**c) Cross-Referencing**
- Build dependency graph of discovered files/modules
- Identify patterns that span research units
- Connect findings into coherent narrative

**d) Gap Analysis**
- Collect all `unanswered_questions` from units
- Determine if gaps are critical or acceptable
- Note areas for future investigation

**e) Evidence Compilation**
- Group findings by category
- Ensure all claims have file:line evidence
- Track confidence levels across findings

### Step 3: Generate Final Report

Create `.claude/work/deep-research/final-report.md`:

```markdown
# Deep Research Report: {Brief}

*Research completed: {date}*
*Units: {N} | Files explored: {M} | Duration: {minutes}*

---

## Executive Summary

[2-3 paragraphs synthesizing the key findings and providing actionable insights]

**Key Takeaways:**
- Most important finding 1
- Most important finding 2
- Most important finding 3

---

## Detailed Findings

### 1. {Category} (e.g., Architecture, Data Flow, Patterns)

**Summary**: [High-level description]

**Evidence**:
- `src/file.ts:45` - JWT tokens generated using jsonwebtoken library
- `lib/redis.ts:78` - Tokens stored in Redis with key pattern "auth:token:{userId}"
- `middleware/auth.ts:23` - Token validation checks signature and expiry

**Confidence**: High | Medium | Low

**Impact**: [Why this matters, what it enables/blocks]

**Related Findings**: [Cross-references to other sections]

[Repeat for each major finding category]

---

## Cross-Cutting Insights

[Architecture patterns, common themes, system-wide observations that emerged across multiple research units]

---

## Dependencies & Risks

**Dependencies Discovered:**
- Component A depends on Component B via X
- ...

**Potential Risks:**
- Identified issue 1 (confidence: medium)
- ...

---

## Recommendations

Based on the research findings:

1. **[Action 1]**
   - Rationale: [Based on finding X, Y, Z]
   - Impact: [What this would improve]
   - Effort: Low | Medium | High

2. **[Action 2]**
   ...

---

## Follow-Up Questions

Questions that merit additional investigation:
1. Question 1 (from unit-3)
2. Question 2 (from unit-5)
...

---

## Research Methodology

**Approach:**
- {N} research units exploring {M} total files
- Parallel investigation using Claude Haiku 4.5
- Evidence-based findings with file:line citations

**Coverage:**
- Modules explored: [list]
- Patterns identified: [list]
- Time period: [if historical analysis]

---

## Appendix: Research Trail

**Unit Summaries:**

### Unit 1: {topic}
- Files explored: [list]
- Key finding: [summary]
- Status: Complete

[Repeat for each unit]

---

**Evidence Index:**
All findings are backed by specific file:line references documented above.
```

### Step 4: Quality Assurance

Before finalizing:

‚úÖ **Completeness Check**
- All research questions answered (or noted as unanswered)
- All units contributed to final report
- No research units missing from synthesis

‚úÖ **Evidence Verification**
- Every claim has file:line citation
- Code snippets support the claims
- Confidence levels are honest

‚úÖ **Coherence Check**
- Report tells a coherent story
- Findings build on each other
- Cross-references are accurate

‚úÖ **Actionability Check**
- Recommendations are specific and evidence-based
- Follow-up questions are clear
- Impact of findings is stated

---

## Success Criteria

This research session is successful when:
- ‚úÖ All research units completed
- ‚úÖ Findings are evidence-based (file:line citations)
- ‚úÖ Contradictions are resolved or acknowledged
- ‚úÖ Report is comprehensive yet focused
- ‚úÖ Recommendations are actionable
- ‚úÖ Total time: 3-10 minutes
- ‚úÖ Total cost: $0.50-$2.00

---

## Context Engineering Strategy

**Why This Architecture Works:**
- **Parallel execution**: 3-8 researchers work simultaneously (3-5x faster)
- **Context isolation**: Each Haiku agent has independent 200K context window
- **Compression**: Haiku agents compress findings to 2K tokens before returning
- **Trust**: Main agent trusts Haiku findings (no re-reading files)
- **Cost efficiency**: Haiku ($1/M in, $5/M out) vs Sonnet ($15/M in, $75/M out)

**Token Budget:**
- Each Haiku researcher: ~50K tokens exploration ‚Üí 2K findings
- Main agent: 12K compressed findings (6 units √ó 2K) + 10K synthesis = 22K
- Total: ~300K exploration, but main agent only sees 22K

---

## Error Handling

**If research unit fails:**
- Read unit's notes file to diagnose
- Adjust scope or provide hints
- Optionally retry with refined questions

**If findings contradict:**
- Present both perspectives with evidence
- Note confidence levels
- Recommend follow-up investigation if critical

**If critical gaps remain:**
- Document in follow-up questions
- Suggest creating new research units
- Note impact of unknown information

**If context overflow:**
- Haiku agents auto-compact notes
- Main agent works with compressed JSONs only
- Full details preserved in notes files

---

## Advanced Features (Optional)

### Debate for Conflicts
If findings strongly contradict:
1. Present findings to a critic subagent
2. Critic examines evidence from both units
3. Researchers provide additional evidence
4. Synthesize with weighted confidence

### Adaptive Depth
Adjust max_depth based on initial findings:
- If unit finds answer quickly ‚Üí reduce depth
- If unit hits complexity ‚Üí increase depth
- Dynamic reallocation of exploration budget

### Follow-Up Research
If gaps are critical:
1. Create new research units targeting gaps
2. Queue additional todos
3. Run second synthesis pass
4. Append to final report

---

## Example Decomposition

**Question**: "How does rate limiting work in our API?"

**Plan** (4 units):
1. **Rate Limit Middleware**: Implementation details
   - Starting: `middleware/rate-limit.ts`
   - Questions: Algorithm? Configurable? Per-route?

2. **Storage Backend**: How counters are tracked
   - Starting: `lib/redis.ts`
   - Questions: Key format? TTL? Cleanup?

3. **API Integration**: Which routes use it
   - Starting: `routes/api/*.ts`
   - Questions: Default limits? Custom limits? Patterns?

4. **Historical Changes**: Evolution of rate limiting
   - Starting: `git log middleware/rate-limit.ts`
   - Questions: Why changed? What issues fixed?

**Expected Output**: Comprehensive understanding of rate limiting with specific evidence about implementation, configuration, and usage patterns.

---

## Tips for Best Results

1. **Start Broad, Then Focus**
   - Initial units should map the terrain
   - Follow-up units dive deep into interesting areas

2. **Trust Your Researchers**
   - Haiku 4.5 is fast and capable
   - Don't second-guess their findings
   - Use their evidence directly

3. **Embrace Uncertainty**
   - Mark confidence levels honestly
   - Document unknowns clearly
   - Note when follow-up needed

4. **Synthesize, Don't Summarize**
   - Connect findings across units
   - Identify patterns and themes
   - Tell a coherent story

5. **Be Actionable**
   - Recommendations should be specific
   - Link back to evidence
   - Consider impact and effort

---

**Ready to start? Let's investigate!** üîç