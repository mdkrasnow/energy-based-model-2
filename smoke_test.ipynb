{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf energy-based-model-2\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model-2\n",
    "!cd energy-based-model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0c280",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport subprocess\nimport argparse\nimport json\nfrom pathlib import Path\nimport time\nimport re\n\n# Hyperparameters from the paper (Appendix A)\nBATCH_SIZE = 2048\nLEARNING_RATE = 1e-4\nTRAIN_ITERATIONS = 1000  # Paper states ~50,000 iterations\nDIFFUSION_STEPS = 10\nRANK = 20  # For 20x20 matrices\n\n# Tasks to run\nTASKS = ['addition']\n\nclass ExperimentRunner:\n    def __init__(self, base_dir='experiments'):\n        self.base_dir = Path(base_dir)\n        self.base_dir.mkdir(exist_ok=True)\n        self.results = {}\n        \n    def get_result_dir(self, dataset):\n        \"\"\"Get the results directory for a given dataset\"\"\"\n        return f'results/ds_{dataset}/model_mlp_diffsteps_{DIFFUSION_STEPS}'\n    \n    def train_model(self, dataset, force_retrain=False):\n        \"\"\"Train a model for a specific dataset\"\"\"\n        result_dir = self.get_result_dir(dataset)\n        \n        # Check if model already exists\n        if not force_retrain and os.path.exists(f'{result_dir}/model-1.pt'):\n            print(f\"\\n{'='*80}\")\n            print(f\"Model for {dataset} already exists. Skipping training.\")\n            print(f\"Use --force to retrain.\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            return True\n            \n        print(f\"\\n{'='*80}\")\n        print(f\"Training IRED on {dataset.upper()} task\")\n        print(f\"{'='*80}\")\n        print(f\"Batch size: {BATCH_SIZE}\")\n        print(f\"Learning rate: {LEARNING_RATE}\")\n        print(f\"Training iterations: {TRAIN_ITERATIONS}\")\n        print(f\"Diffusion steps: {DIFFUSION_STEPS}\")\n        print(f\"Matrix rank: {RANK}\")\n        print(f\"Result directory: {result_dir}\")\n        print(f\"{'='*80}\\n\")\n        sys.stdout.flush()\n        \n        # Build command\n        cmd = [\n            'python', 'train.py',\n            '--dataset', dataset,\n            '--model', 'mlp',\n            '--batch_size', str(BATCH_SIZE),\n            '--diffusion_steps', str(DIFFUSION_STEPS),\n            '--rank', str(RANK),\n        ]\n        \n        # Run training with real-time output\n        try:\n            start_time = time.time()\n            \n            # Use subprocess.Popen for real-time output with flushing\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n                bufsize=1\n            )\n            \n            # Display output line by line as it comes\n            for line in iter(process.stdout.readline, ''):\n                if line:\n                    print(line.rstrip())\n                    sys.stdout.flush()\n            \n            # Wait for process to complete\n            result = process.wait()\n            elapsed = time.time() - start_time\n            \n            if result == 0:\n                print(f\"\\n{'='*80}\")\n                print(f\"Training completed for {dataset} in {elapsed/3600:.2f} hours\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                return True\n            else:\n                print(f\"\\n{'='*80}\")\n                print(f\"ERROR: Training failed for {dataset} with exit code {result}\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                return False\n            \n        except Exception as e:\n            print(f\"\\n{'='*80}\")\n            print(f\"ERROR: Training failed for {dataset}: {e}\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            return False\n    \n    def evaluate_model(self, dataset, ood=False):\n        \"\"\"Evaluate a trained model on same or harder difficulty\"\"\"\n        result_dir = self.get_result_dir(dataset)\n        \n        # Check if model exists\n        if not os.path.exists(f'{result_dir}/model-1.pt'):\n            print(f\"\\n{'='*80}\")\n            print(f\"ERROR: No trained model found for {dataset}\")\n            print(f\"Expected location: {result_dir}/model-1.pt\")\n            print(f\"Please train the model first.\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            return None\n        \n        difficulty = \"Harder Difficulty (OOD)\" if ood else \"Same Difficulty\"\n        print(f\"\\n{'='*80}\")\n        print(f\"Evaluating IRED on {dataset.upper()} - {difficulty}\")\n        print(f\"{'='*80}\\n\")\n        sys.stdout.flush()\n        \n        # Build command\n        cmd = [\n            'python', 'train.py',\n            '--dataset', dataset,\n            '--model', 'mlp',\n            '--batch_size', str(BATCH_SIZE),\n            '--diffusion_steps', str(DIFFUSION_STEPS),\n            '--rank', str(RANK),\n            '--load-milestone', '1',\n            '--evaluate',\n        ]\n        \n        if ood:\n            cmd.append('--ood')\n        \n        # Run evaluation with real-time output\n        try:\n            # Collect output for MSE parsing while also displaying it\n            output_lines = []\n            \n            # Use subprocess.Popen for real-time output with flushing\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n                bufsize=1\n            )\n            \n            # Display output line by line as it comes\n            for line in iter(process.stdout.readline, ''):\n                if line:\n                    print(line.rstrip())\n                    sys.stdout.flush()\n                    output_lines.append(line)\n            \n            # Wait for process to complete\n            result = process.wait()\n            \n            if result == 0:\n                # Parse output to extract MSE\n                output_text = ''.join(output_lines)\n                mse = self._parse_mse_from_output(output_text, '')\n                \n                print(f\"\\n{'='*80}\")\n                print(f\"Evaluation completed for {dataset} - {difficulty}\")\n                if mse is not None:\n                    print(f\"MSE: {mse:.4f}\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                \n                return mse\n            else:\n                print(f\"\\n{'='*80}\")\n                print(f\"ERROR: Evaluation failed for {dataset} - {difficulty} with exit code {result}\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                return None\n            \n        except Exception as e:\n            print(f\"\\n{'='*80}\")\n            print(f\"ERROR: Evaluation failed for {dataset} - {difficulty}: {e}\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            return None\n    \n    def _parse_mse_from_output(self, stdout, stderr):\n        \"\"\"Parse MSE from training/evaluation output\"\"\"\n        output = stdout + stderr\n        lines = output.split('\\n')\n        \n        # Look for validation result tables with MSE values\n        # Format:\n        # ---  --------\n        # mse  0.636725\n        # ---  --------\n        \n        mse_value = None\n        for i, line in enumerate(lines):\n            # Look for the specific pattern of mse in a table\n            if line.startswith('mse') and '  ' in line:\n                # This looks like a table row with MSE\n                parts = line.split()\n                if len(parts) >= 2 and parts[0] == 'mse':\n                    try:\n                        mse_value = float(parts[1])\n                        # Continue searching to find the last MSE value (most recent)\n                    except (ValueError, IndexError):\n                        pass\n        \n        # If we didn't find MSE in table format, try alternative formats\n        if mse_value is None:\n            # Look for patterns like \"mse_error  0.635722\"\n            for line in lines:\n                if 'mse_error' in line.lower():\n                    parts = line.split()\n                    for i, part in enumerate(parts):\n                        if 'mse' in part.lower() and i + 1 < len(parts):\n                            try:\n                                mse_value = float(parts[i + 1])\n                            except ValueError:\n                                pass\n        \n        return mse_value\n    \n    def train_all(self, force_retrain=False):\n        \"\"\"Train all models\"\"\"\n        print(f\"\\n{'#'*80}\")\n        print(f\"# TRAINING ALL CONTINUOUS TASKS\")\n        print(f\"# Tasks: {', '.join(TASKS)}\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n        \n        success = {}\n        for dataset in TASKS:\n            success[dataset] = self.train_model(dataset, force_retrain)\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# TRAINING SUMMARY\")\n        print(f\"{'#'*80}\")\n        for dataset, status in success.items():\n            status_str = \"✓ SUCCESS\" if status else \"✗ FAILED\"\n            print(f\"{dataset:20s}: {status_str}\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n        \n        return all(success.values())\n    \n    def evaluate_all(self):\n        \"\"\"Evaluate all models on both same and harder difficulty\"\"\"\n        print(f\"\\n{'#'*80}\")\n        print(f\"# EVALUATING ALL CONTINUOUS TASKS\")\n        print(f\"# Tasks: {', '.join(TASKS)}\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n        \n        results = {}\n        for dataset in TASKS:\n            results[dataset] = {\n                'same_difficulty': self.evaluate_model(dataset, ood=False),\n                'harder_difficulty': self.evaluate_model(dataset, ood=True)\n            }\n        \n        self.results = results\n        self._print_results_table()\n        self._save_results()\n        \n        return results\n    \n    def _print_results_table(self):\n        \"\"\"Print results in a table format like Table 1\"\"\"\n        print(f\"\\n{'#'*80}\")\n        print(f\"# RESULTS TABLE (Replicating Table 1 from IRED Paper)\")\n        print(f\"{'#'*80}\\n\")\n        \n        # Print header\n        print(f\"{'Task':<20s} {'Method':<15s} {'Same Difficulty':>15s} {'Harder Difficulty':>17s}\")\n        print(f\"{'-'*20} {'-'*15} {'-'*15} {'-'*17}\")\n        \n        # Task name mapping for display\n        task_display = {\n            'addition': 'Addition',\n            'lowrank': 'Matrix Completion',\n            'inverse': 'Matrix Inverse'\n        }\n        \n        # Print results for each task\n        for dataset in TASKS:\n            task_name = task_display.get(dataset, dataset)\n            same = self.results.get(dataset, {}).get('same_difficulty')\n            harder = self.results.get(dataset, {}).get('harder_difficulty')\n            \n            same_str = f\"{same:.4f}\" if same is not None else \"N/A\"\n            harder_str = f\"{harder:.4f}\" if harder is not None else \"N/A\"\n            \n            print(f\"{task_name:<20s} {'IRED (ours)':<15s} {same_str:>15s} {harder_str:>17s}\")\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# Paper's reported IRED results for comparison:\")\n        print(f\"{'#'*80}\")\n        print(f\"{'Addition':<20s} {'IRED (paper)':<15s} {'0.0002':>15s} {'0.0020':>17s}\")\n        print(f\"{'Matrix Completion':<20s} {'IRED (paper)':<15s} {'0.0174':>15s} {'0.2054':>17s}\")\n        print(f\"{'Matrix Inverse':<20s} {'IRED (paper)':<15s} {'0.0095':>15s} {'0.2063':>17s}\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n    \n    def _save_results(self):\n        \"\"\"Save results to JSON file\"\"\"\n        results_file = self.base_dir / 'continuous_results.json'\n        \n        # Add metadata\n        results_with_meta = {\n            'metadata': {\n                'batch_size': BATCH_SIZE,\n                'learning_rate': LEARNING_RATE,\n                'train_iterations': TRAIN_ITERATIONS,\n                'diffusion_steps': DIFFUSION_STEPS,\n                'rank': RANK,\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')\n            },\n            'results': self.results\n        }\n        \n        with open(results_file, 'w') as f:\n            json.dump(results_with_meta, f, indent=2)\n        \n        print(f\"Results saved to: {results_file}\\n\")\n        sys.stdout.flush()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988de22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize runner with base directory\n",
    "args = argparse.Namespace(base_dir='experiments', force=False)\n",
    "runner = ExperimentRunner(base_dir=args.base_dir)\n",
    "\n",
    "# Train all models\n",
    "success = runner.train_all(force_retrain=args.force)\n",
    "\n",
    "# Evaluate if training succeeded\n",
    "if success:\n",
    "    # Evaluate all\n",
    "    runner.evaluate_all()\n",
    "else:\n",
    "    print(\"\\nSome training jobs failed. Skipping evaluation.\")\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}