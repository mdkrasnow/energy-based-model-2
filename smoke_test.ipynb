{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf energy-based-model-2\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model-2\n",
    "%cd energy-based-model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision einops accelerate tqdm tabulate matplotlib numpy pandas ema-pytorch ipdb seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0c280",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport subprocess\nimport argparse\nimport json\nfrom pathlib import Path\nimport time\nimport re\n\n# Hyperparameters from the paper (Appendix A)\nBATCH_SIZE = 2048\nLEARNING_RATE = 1e-4\nTRAIN_ITERATIONS = 1000  \nDIFFUSION_STEPS = 10\nRANK = 20  # For 20x20 matrices\n\n# Tasks to run\nTASKS = ['addition']\n\nclass ExperimentRunner:\n    def __init__(self, base_dir='experiments'):\n        self.base_dir = Path(base_dir)\n        self.base_dir.mkdir(exist_ok=True)\n        self.results = {}\n        \n    def get_result_dir(self, dataset, model_type='baseline'):\n        \"\"\"Get the results directory for a given dataset and model type\"\"\"\n        base = f'results/ds_{dataset}/model_mlp_diffsteps_{DIFFUSION_STEPS}'\n        if model_type == 'anm':\n            base += '_anm_curriculum'  # ANM always uses curriculum\n        return base\n    \n    def train_model(self, dataset, model_type='baseline', force_retrain=False):\n        \"\"\"Train a model for a specific dataset and model type\n        \n        Args:\n            dataset: Dataset name\n            model_type: One of 'baseline', 'anm' (which always uses curriculum)\n            force_retrain: Force retraining even if model exists\n        \"\"\"\n        result_dir = self.get_result_dir(dataset, model_type)\n        \n        # Check if model already exists\n        if not force_retrain and os.path.exists(f'{result_dir}/model-1.pt'):\n            print(f\"\\n{'='*80}\")\n            print(f\"Model for {dataset} ({model_type}) already exists. Skipping training.\")\n            print(f\"Use --force to retrain.\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            return True\n            \n        print(f\"\\n{'='*80}\")\n        print(f\"Training IRED ({model_type.upper()}) on {dataset.upper()} task\")\n        print(f\"{'='*80}\")\n        print(f\"Model Type: {model_type}\")\n        print(f\"Batch size: {BATCH_SIZE}\")\n        print(f\"Learning rate: {LEARNING_RATE}\")\n        print(f\"Training iterations: {TRAIN_ITERATIONS}\")\n        print(f\"Diffusion steps: {DIFFUSION_STEPS}\")\n        print(f\"Matrix rank: {RANK}\")\n        print(f\"Result directory: {result_dir}\")\n        \n        if model_type == 'anm':\n            print(f\"\\nANM with AGGRESSIVE Curriculum Schedule (% of {TRAIN_ITERATIONS} steps):\")\n            print(f\"  Warmup (0-10%): 100% clean, 0% adversarial, ε=0.0\")\n            print(f\"  Rapid Introduction (10-25%): 50% clean, 40% adversarial, 10% gaussian, ε=0.3\")\n            print(f\"  Aggressive Ramp (25-50%): 20% clean, 70% adversarial, 10% gaussian, ε=0.7\")\n            print(f\"  High Intensity (50-80%): 10% clean, 85% adversarial, 5% gaussian, ε=1.0\")\n            print(f\"  Extreme Hardening (80-100%): 5% clean, 90% adversarial, 5% gaussian, ε=1.2\")\n            \n        print(f\"{'='*80}\\n\")\n        sys.stdout.flush()\n        \n        # Build command\n        cmd = [\n            'python', 'train.py',\n            '--dataset', dataset,\n            '--model', 'mlp',\n            '--batch_size', str(BATCH_SIZE),\n            '--diffusion_steps', str(DIFFUSION_STEPS),\n            '--rank', str(RANK),\n            '--train-steps', str(TRAIN_ITERATIONS),  # Pass training steps\n        ]\n        \n        # Add model-specific parameters\n        if model_type == 'anm':\n            cmd.extend([\n                '--use-anm',\n                '--anm-adversarial-steps', '5',\n                '--anm-distance-penalty', '0.1',\n                # ANM now always uses curriculum, no need for --use-curriculum flag\n            ])\n        \n        # Run training with real-time output\n        try:\n            start_time = time.time()\n            \n            # Use subprocess.Popen for real-time output with flushing\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n                bufsize=1\n            )\n            \n            # Display output line by line as it comes\n            for line in iter(process.stdout.readline, ''):\n                if line:\n                    print(line.rstrip())\n                    sys.stdout.flush()\n            \n            # Wait for process to complete\n            result = process.wait()\n            elapsed = time.time() - start_time\n            \n            if result == 0:\n                print(f\"\\n{'='*80}\")\n                print(f\"Training completed for {dataset} ({model_type}) in {elapsed/60:.2f} minutes\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                return True\n            else:\n                print(f\"\\n{'='*80}\")\n                print(f\"ERROR: Training failed for {dataset} ({model_type}) with exit code {result}\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                return False\n            \n        except Exception as e:\n            print(f\"\\n{'='*80}\")\n            print(f\"ERROR: Training failed for {dataset} ({model_type}): {e}\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            return False\n    \n    def evaluate_model(self, dataset, model_type='baseline', ood=False):\n        \"\"\"Evaluate a trained model on same or harder difficulty\"\"\"\n        result_dir = self.get_result_dir(dataset, model_type)\n        \n        # Check if model exists\n        if not os.path.exists(f'{result_dir}/model-1.pt'):\n            print(f\"\\n{'='*80}\")\n            print(f\"ERROR: No trained model found for {dataset} ({model_type})\")\n            print(f\"Expected location: {result_dir}/model-1.pt\")\n            print(f\"Please train the model first.\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            return None\n        \n        difficulty = \"Harder Difficulty (OOD)\" if ood else \"Same Difficulty\"\n        print(f\"\\n{'='*80}\")\n        print(f\"Evaluating IRED ({model_type.upper()}) on {dataset.upper()} - {difficulty}\")\n        print(f\"{'='*80}\\n\")\n        sys.stdout.flush()\n        \n        # Build command\n        cmd = [\n            'python', 'train.py',\n            '--dataset', dataset,\n            '--model', 'mlp',\n            '--batch_size', str(BATCH_SIZE),\n            '--diffusion_steps', str(DIFFUSION_STEPS),\n            '--rank', str(RANK),\n            '--train-steps', str(TRAIN_ITERATIONS),  # Pass for consistency\n            '--load-milestone', '1',\n            '--evaluate',\n        ]\n        \n        # Add model-specific parameters for evaluation\n        if model_type == 'anm':\n            cmd.extend([\n                '--use-anm',\n                '--anm-adversarial-steps', '5',\n                '--anm-distance-penalty', '0.1',\n                # ANM now always uses curriculum, no need for --use-curriculum flag\n            ])\n        \n        if ood:\n            cmd.append('--ood')\n        \n        # Run evaluation with real-time output\n        try:\n            # Collect output for MSE parsing while also displaying it\n            output_lines = []\n            \n            # Use subprocess.Popen for real-time output with flushing\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n                bufsize=1\n            )\n            \n            # Display output line by line as it comes\n            for line in iter(process.stdout.readline, ''):\n                if line:\n                    print(line.rstrip())\n                    sys.stdout.flush()\n                    output_lines.append(line)\n            \n            # Wait for process to complete\n            result = process.wait()\n            \n            if result == 0:\n                # Parse output to extract MSE\n                output_text = ''.join(output_lines)\n                mse = self._parse_mse_from_output(output_text, '')\n                \n                print(f\"\\n{'='*80}\")\n                print(f\"Evaluation completed for {dataset} ({model_type}) - {difficulty}\")\n                if mse is not None:\n                    print(f\"MSE: {mse:.4f}\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                \n                return mse\n            else:\n                print(f\"\\n{'='*80}\")\n                print(f\"ERROR: Evaluation failed for {dataset} ({model_type}) - {difficulty} with exit code {result}\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                return None\n            \n        except Exception as e:\n            print(f\"\\n{'='*80}\")\n            print(f\"ERROR: Evaluation failed for {dataset} ({model_type}) - {difficulty}: {e}\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            return None\n    \n    def _parse_mse_from_output(self, stdout, stderr):\n        \"\"\"Parse MSE from training/evaluation output\"\"\"\n        output = stdout + stderr\n        lines = output.split('\\n')\n        \n        # Look for validation result tables with MSE values\n        mse_value = None\n        for i, line in enumerate(lines):\n            # Look for the specific pattern of mse in a table\n            if line.startswith('mse') and '  ' in line:\n                # This looks like a table row with MSE\n                parts = line.split()\n                if len(parts) >= 2 and parts[0] == 'mse':\n                    try:\n                        mse_value = float(parts[1])\n                        # Continue searching to find the last MSE value (most recent)\n                    except (ValueError, IndexError):\n                        pass\n        \n        # If we didn't find MSE in table format, try alternative formats\n        if mse_value is None:\n            # Look for patterns like \"mse_error  0.635722\"\n            for line in lines:\n                if 'mse_error' in line.lower():\n                    parts = line.split()\n                    for i, part in enumerate(parts):\n                        if 'mse' in part.lower() and i + 1 < len(parts):\n                            try:\n                                mse_value = float(parts[i + 1])\n                            except ValueError:\n                                pass\n        \n        return mse_value\n    \n    def train_all(self, force_retrain=False):\n        \"\"\"Train all models (baseline and ANM with curriculum)\"\"\"\n        print(f\"\\n{'#'*80}\")\n        print(f\"# TRAINING ALL CONTINUOUS TASKS\")\n        print(f\"# Tasks: {', '.join(TASKS)}\")\n        print(f\"# Model Types: baseline, anm (with curriculum)\")\n        print(f\"# Training Steps: {TRAIN_ITERATIONS}\")\n        print(f\"# ANM uses AGGRESSIVE curriculum (predefined from curriculum_config.py)\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n        \n        success = {}\n        model_types = ['baseline', 'anm']\n        \n        for dataset in TASKS:\n            for model_type in model_types:\n                key = f\"{dataset}_{model_type}\"\n                success[key] = self.train_model(dataset, model_type, force_retrain)\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# TRAINING SUMMARY\")\n        print(f\"{'#'*80}\")\n        for dataset in TASKS:\n            print(f\"\\n{dataset.upper()}:\")\n            for model_type in model_types:\n                key = f\"{dataset}_{model_type}\"\n                status_str = \"✓ SUCCESS\" if success.get(key, False) else \"✗ FAILED\"\n                model_desc = \"ANM+Curriculum\" if model_type == 'anm' else model_type\n                print(f\"  {model_desc:20s}: {status_str}\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n        \n        return all(success.values())\n    \n    def evaluate_all(self):\n        \"\"\"Evaluate all models on both same and harder difficulty\"\"\"\n        print(f\"\\n{'#'*80}\")\n        print(f\"# EVALUATING ALL CONTINUOUS TASKS\")\n        print(f\"# Tasks: {', '.join(TASKS)}\")\n        print(f\"# Model Types: baseline, anm (with curriculum)\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n        \n        results = {}\n        model_types = ['baseline', 'anm']\n        \n        for dataset in TASKS:\n            results[dataset] = {}\n            for model_type in model_types:\n                results[dataset][model_type] = {\n                    'same_difficulty': self.evaluate_model(dataset, model_type, ood=False),\n                    'harder_difficulty': self.evaluate_model(dataset, model_type, ood=True)\n                }\n        \n        self.results = results\n        self._print_results_table()\n        self._save_results()\n        \n        return results\n    \n    def _print_results_table(self):\n        \"\"\"Print results in a comparison table format\"\"\"\n        print(f\"\\n{'#'*80}\")\n        print(f\"# RESULTS COMPARISON TABLE\")\n        print(f\"# Training Steps: {TRAIN_ITERATIONS}\")\n        print(f\"{'#'*80}\\n\")\n        \n        # Print header\n        print(f\"{'Task':<20s} {'Method':<25s} {'Same Difficulty':>15s} {'Harder Difficulty':>17s}\")\n        print(f\"{'-'*20} {'-'*25} {'-'*15} {'-'*17}\")\n        \n        # Task name mapping for display\n        task_display = {\n            'addition': 'Addition',\n            'lowrank': 'Matrix Completion',\n            'inverse': 'Matrix Inverse'\n        }\n        \n        # Print results for each task\n        for dataset in TASKS:\n            task_name = task_display.get(dataset, dataset)\n            \n            # Baseline\n            baseline_same = self.results.get(dataset, {}).get('baseline', {}).get('same_difficulty')\n            baseline_harder = self.results.get(dataset, {}).get('baseline', {}).get('harder_difficulty')\n            baseline_same_str = f\"{baseline_same:.4f}\" if baseline_same is not None else \"N/A\"\n            baseline_harder_str = f\"{baseline_harder:.4f}\" if baseline_harder is not None else \"N/A\"\n            print(f\"{task_name:<20s} {'IRED (baseline)':<25s} {baseline_same_str:>15s} {baseline_harder_str:>17s}\")\n            \n            # ANM with curriculum\n            anm_same = self.results.get(dataset, {}).get('anm', {}).get('same_difficulty')\n            anm_harder = self.results.get(dataset, {}).get('anm', {}).get('harder_difficulty')\n            anm_same_str = f\"{anm_same:.4f}\" if anm_same is not None else \"N/A\"\n            anm_harder_str = f\"{anm_harder:.4f}\" if anm_harder is not None else \"N/A\"\n            print(f\"{'':<20s} {'IRED + ANM (curriculum)':<25s} {anm_same_str:>15s} {anm_harder_str:>17s}\")\n            \n            print()  # Blank line between tasks\n        \n        # Print improvement percentages if baseline exists\n        print(f\"\\n{'#'*80}\")\n        print(f\"# RELATIVE IMPROVEMENTS vs BASELINE\")\n        print(f\"{'#'*80}\\n\")\n        \n        for dataset in TASKS:\n            task_name = task_display.get(dataset, dataset)\n            baseline_same = self.results.get(dataset, {}).get('baseline', {}).get('same_difficulty')\n            baseline_harder = self.results.get(dataset, {}).get('baseline', {}).get('harder_difficulty')\n            \n            if baseline_same and baseline_harder:\n                print(f\"{task_name}:\")\n                \n                # ANM improvements\n                anm_same = self.results.get(dataset, {}).get('anm', {}).get('same_difficulty')\n                anm_harder = self.results.get(dataset, {}).get('anm', {}).get('harder_difficulty')\n                if anm_same and anm_harder:\n                    same_imp = ((baseline_same - anm_same) / baseline_same) * 100\n                    harder_imp = ((baseline_harder - anm_harder) / baseline_harder) * 100\n                    print(f\"  ANM+Curriculum: {same_imp:+.1f}% (same), {harder_imp:+.1f}% (harder)\")\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# Paper's reported IRED results for comparison:\")\n        print(f\"{'#'*80}\")\n        print(f\"{'Addition':<20s} {'IRED (paper)':<25s} {'0.0002':>15s} {'0.0020':>17s}\")\n        print(f\"{'Matrix Completion':<20s} {'IRED (paper)':<25s} {'0.0174':>15s} {'0.2054':>17s}\")\n        print(f\"{'Matrix Inverse':<20s} {'IRED (paper)':<25s} {'0.0095':>15s} {'0.2063':>17s}\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n    \n    def _save_results(self):\n        \"\"\"Save results to JSON file\"\"\"\n        results_file = self.base_dir / 'continuous_results_with_anm.json'\n        \n        # Add metadata\n        results_with_meta = {\n            'metadata': {\n                'batch_size': BATCH_SIZE,\n                'learning_rate': LEARNING_RATE,\n                'train_iterations': TRAIN_ITERATIONS,\n                'diffusion_steps': DIFFUSION_STEPS,\n                'rank': RANK,\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n                'model_types': ['baseline', 'anm'],\n                'anm_note': 'ANM always uses curriculum learning',\n                'curriculum': {\n                    'type': 'AGGRESSIVE_CURRICULUM',\n                    'warmup': '0-10% (100% clean, ε=0.0)',\n                    'rapid_introduction': '10-25% (50% clean, 40% adversarial, ε=0.3)',\n                    'aggressive_ramp': '25-50% (20% clean, 70% adversarial, ε=0.7)',\n                    'high_intensity': '50-80% (10% clean, 85% adversarial, ε=1.0)',\n                    'extreme_hardening': '80-100% (5% clean, 90% adversarial, ε=1.2)'\n                }\n            },\n            'results': self.results\n        }\n        \n        with open(results_file, 'w') as f:\n            json.dump(results_with_meta, f, indent=2)\n        \n        print(f\"Results saved to: {results_file}\\n\")\n        sys.stdout.flush()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988de22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize runner with base directory\n",
    "args = argparse.Namespace(base_dir='experiments', force=False)\n",
    "runner = ExperimentRunner(base_dir=args.base_dir)\n",
    "\n",
    "# Train all models\n",
    "success = runner.train_all(force_retrain=args.force)\n",
    "\n",
    "# Evaluate if training succeeded\n",
    "if success:\n",
    "    # Evaluate all\n",
    "    runner.evaluate_all()\n",
    "else:\n",
    "    print(\"\\nSome training jobs failed. Skipping evaluation.\")\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}