{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf energy-based-model-2\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model-2\n",
    "%cd energy-based-model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision einops accelerate tqdm tabulate matplotlib numpy pandas ema-pytorch ipdb seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0c280",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import argparse\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Hyperparameters from the paper (Appendix A)\n",
    "BATCH_SIZE = 2048\n",
    "LEARNING_RATE = 1e-4\n",
    "TRAIN_ITERATIONS = 1000  \n",
    "DIFFUSION_STEPS = 10\n",
    "RANK = 20  # For 20x20 matrices\n",
    "\n",
    "# Tasks to run\n",
    "TASKS = ['addition']\n",
    "\n",
    "class ExperimentRunner:\n",
    "    def __init__(self, base_dir='experiments'):\n",
    "        self.base_dir = Path(base_dir)\n",
    "        self.base_dir.mkdir(exist_ok=True)\n",
    "        self.results = {}\n",
    "        \n",
    "    def get_result_dir(self, dataset, model_type='baseline'):\n",
    "        \"\"\"Get the results directory for a given dataset and model type\"\"\"\n",
    "        base = f'results/ds_{dataset}/model_mlp_diffsteps_{DIFFUSION_STEPS}'\n",
    "        if model_type == 'anm':\n",
    "            base += '_anm'\n",
    "        elif model_type == 'anm_curriculum':\n",
    "            base += '_anm_curriculum'\n",
    "        return base\n",
    "    \n",
    "    def train_model(self, dataset, model_type='baseline', force_retrain=False):\n",
    "        \"\"\"Train a model for a specific dataset and model type\n",
    "        \n",
    "        Args:\n",
    "            dataset: Dataset name\n",
    "            model_type: One of 'baseline', 'anm', 'anm_curriculum'\n",
    "            force_retrain: Force retraining even if model exists\n",
    "        \"\"\"\n",
    "        result_dir = self.get_result_dir(dataset, model_type)\n",
    "        \n",
    "        # Check if model already exists\n",
    "        if not force_retrain and os.path.exists(f'{result_dir}/model-1.pt'):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"Model for {dataset} ({model_type}) already exists. Skipping training.\")\n",
    "            print(f\"Use --force to retrain.\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            sys.stdout.flush()\n",
    "            return True\n",
    "            \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Training IRED ({model_type.upper()}) on {dataset.upper()} task\")\n",
    "        print(f\"{'='*80}\")\n",
    "        print(f\"Model Type: {model_type}\")\n",
    "        print(f\"Batch size: {BATCH_SIZE}\")\n",
    "        print(f\"Learning rate: {LEARNING_RATE}\")\n",
    "        print(f\"Training iterations: {TRAIN_ITERATIONS}\")\n",
    "        print(f\"Diffusion steps: {DIFFUSION_STEPS}\")\n",
    "        print(f\"Matrix rank: {RANK}\")\n",
    "        print(f\"Result directory: {result_dir}\")\n",
    "        \n",
    "        if model_type == 'anm_curriculum':\n",
    "            print(f\"\\nCurriculum Schedule (% of {TRAIN_ITERATIONS} steps):\")\n",
    "            print(f\"  Warmup (0-10%): Clean samples only\")\n",
    "            print(f\"  Easy (10-30%): 50% clean, 30% adversarial, 20% gaussian\")\n",
    "            print(f\"  Medium (30-60%): 30% clean, 50% adversarial, 20% gaussian\")\n",
    "            print(f\"  Hard (60-100%): 10% clean, 80% adversarial, 10% gaussian\")\n",
    "            \n",
    "        print(f\"{'='*80}\\n\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Build command\n",
    "        cmd = [\n",
    "            'python', 'train.py',\n",
    "            '--dataset', dataset,\n",
    "            '--model', 'mlp',\n",
    "            '--batch_size', str(BATCH_SIZE),\n",
    "            '--diffusion_steps', str(DIFFUSION_STEPS),\n",
    "            '--rank', str(RANK),\n",
    "            '--train-steps', str(TRAIN_ITERATIONS),  # Pass training steps\n",
    "        ]\n",
    "        \n",
    "        # Add model-specific parameters\n",
    "        if model_type == 'anm':\n",
    "            cmd.extend([\n",
    "                '--use-anm',\n",
    "                '--anm-adversarial-steps', '5',\n",
    "                '--anm-distance-penalty', '0.1',\n",
    "                # anm_warmup_steps will be calculated as 10% of train_steps automatically\n",
    "            ])\n",
    "        elif model_type == 'anm_curriculum':\n",
    "            cmd.extend([\n",
    "                '--use-anm',\n",
    "                '--use-curriculum',\n",
    "                '--anm-adversarial-steps', '5',\n",
    "                '--anm-distance-penalty', '0.1',\n",
    "                # Curriculum and warmup will be percentage-based\n",
    "            ])\n",
    "        \n",
    "        # Run training with real-time output\n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Use subprocess.Popen for real-time output with flushing\n",
    "            process = subprocess.Popen(\n",
    "                cmd,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True,\n",
    "                bufsize=1\n",
    "            )\n",
    "            \n",
    "            # Display output line by line as it comes\n",
    "            for line in iter(process.stdout.readline, ''):\n",
    "                if line:\n",
    "                    print(line.rstrip())\n",
    "                    sys.stdout.flush()\n",
    "            \n",
    "            # Wait for process to complete\n",
    "            result = process.wait()\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            if result == 0:\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"Training completed for {dataset} ({model_type}) in {elapsed/60:.2f} minutes\")\n",
    "                print(f\"{'='*80}\\n\")\n",
    "                sys.stdout.flush()\n",
    "                return True\n",
    "            else:\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"ERROR: Training failed for {dataset} ({model_type}) with exit code {result}\")\n",
    "                print(f\"{'='*80}\\n\")\n",
    "                sys.stdout.flush()\n",
    "                return False\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"ERROR: Training failed for {dataset} ({model_type}): {e}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            sys.stdout.flush()\n",
    "            return False\n",
    "    \n",
    "    def evaluate_model(self, dataset, model_type='baseline', ood=False):\n",
    "        \"\"\"Evaluate a trained model on same or harder difficulty\"\"\"\n",
    "        result_dir = self.get_result_dir(dataset, model_type)\n",
    "        \n",
    "        # Check if model exists\n",
    "        if not os.path.exists(f'{result_dir}/model-1.pt'):\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"ERROR: No trained model found for {dataset} ({model_type})\")\n",
    "            print(f\"Expected location: {result_dir}/model-1.pt\")\n",
    "            print(f\"Please train the model first.\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            sys.stdout.flush()\n",
    "            return None\n",
    "        \n",
    "        difficulty = \"Harder Difficulty (OOD)\" if ood else \"Same Difficulty\"\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Evaluating IRED ({model_type.upper()}) on {dataset.upper()} - {difficulty}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        # Build command\n",
    "        cmd = [\n",
    "            'python', 'train.py',\n",
    "            '--dataset', dataset,\n",
    "            '--model', 'mlp',\n",
    "            '--batch_size', str(BATCH_SIZE),\n",
    "            '--diffusion_steps', str(DIFFUSION_STEPS),\n",
    "            '--rank', str(RANK),\n",
    "            '--train-steps', str(TRAIN_ITERATIONS),  # Pass for consistency\n",
    "            '--load-milestone', '1',\n",
    "            '--evaluate',\n",
    "        ]\n",
    "        \n",
    "        # Add model-specific parameters for evaluation\n",
    "        if model_type == 'anm':\n",
    "            cmd.extend([\n",
    "                '--use-anm',\n",
    "                '--anm-adversarial-steps', '5',\n",
    "                '--anm-distance-penalty', '0.1',\n",
    "            ])\n",
    "        elif model_type == 'anm_curriculum':\n",
    "            cmd.extend([\n",
    "                '--use-anm',\n",
    "                '--use-curriculum',\n",
    "                '--anm-adversarial-steps', '5',\n",
    "                '--anm-distance-penalty', '0.1',\n",
    "            ])\n",
    "        \n",
    "        if ood:\n",
    "            cmd.append('--ood')\n",
    "        \n",
    "        # Run evaluation with real-time output\n",
    "        try:\n",
    "            # Collect output for MSE parsing while also displaying it\n",
    "            output_lines = []\n",
    "            \n",
    "            # Use subprocess.Popen for real-time output with flushing\n",
    "            process = subprocess.Popen(\n",
    "                cmd,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                universal_newlines=True,\n",
    "                bufsize=1\n",
    "            )\n",
    "            \n",
    "            # Display output line by line as it comes\n",
    "            for line in iter(process.stdout.readline, ''):\n",
    "                if line:\n",
    "                    print(line.rstrip())\n",
    "                    sys.stdout.flush()\n",
    "                    output_lines.append(line)\n",
    "            \n",
    "            # Wait for process to complete\n",
    "            result = process.wait()\n",
    "            \n",
    "            if result == 0:\n",
    "                # Parse output to extract MSE\n",
    "                output_text = ''.join(output_lines)\n",
    "                mse = self._parse_mse_from_output(output_text, '')\n",
    "                \n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"Evaluation completed for {dataset} ({model_type}) - {difficulty}\")\n",
    "                if mse is not None:\n",
    "                    print(f\"MSE: {mse:.4f}\")\n",
    "                print(f\"{'='*80}\\n\")\n",
    "                sys.stdout.flush()\n",
    "                \n",
    "                return mse\n",
    "            else:\n",
    "                print(f\"\\n{'='*80}\")\n",
    "                print(f\"ERROR: Evaluation failed for {dataset} ({model_type}) - {difficulty} with exit code {result}\")\n",
    "                print(f\"{'='*80}\\n\")\n",
    "                sys.stdout.flush()\n",
    "                return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"\\n{'='*80}\")\n",
    "            print(f\"ERROR: Evaluation failed for {dataset} ({model_type}) - {difficulty}: {e}\")\n",
    "            print(f\"{'='*80}\\n\")\n",
    "            sys.stdout.flush()\n",
    "            return None\n",
    "    \n",
    "    def _parse_mse_from_output(self, stdout, stderr):\n",
    "        \"\"\"Parse MSE from training/evaluation output\"\"\"\n",
    "        output = stdout + stderr\n",
    "        lines = output.split('\\n')\n",
    "        \n",
    "        # Look for validation result tables with MSE values\n",
    "        mse_value = None\n",
    "        for i, line in enumerate(lines):\n",
    "            # Look for the specific pattern of mse in a table\n",
    "            if line.startswith('mse') and '  ' in line:\n",
    "                # This looks like a table row with MSE\n",
    "                parts = line.split()\n",
    "                if len(parts) >= 2 and parts[0] == 'mse':\n",
    "                    try:\n",
    "                        mse_value = float(parts[1])\n",
    "                        # Continue searching to find the last MSE value (most recent)\n",
    "                    except (ValueError, IndexError):\n",
    "                        pass\n",
    "        \n",
    "        # If we didn't find MSE in table format, try alternative formats\n",
    "        if mse_value is None:\n",
    "            # Look for patterns like \"mse_error  0.635722\"\n",
    "            for line in lines:\n",
    "                if 'mse_error' in line.lower():\n",
    "                    parts = line.split()\n",
    "                    for i, part in enumerate(parts):\n",
    "                        if 'mse' in part.lower() and i + 1 < len(parts):\n",
    "                            try:\n",
    "                                mse_value = float(parts[i + 1])\n",
    "                            except ValueError:\n",
    "                                pass\n",
    "        \n",
    "        return mse_value\n",
    "    \n",
    "    def train_all(self, force_retrain=False):\n",
    "        \"\"\"Train all models (baseline and ANM variants)\"\"\"\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"# TRAINING ALL CONTINUOUS TASKS WITH BASELINE AND ANM MODELS\")\n",
    "        print(f\"# Tasks: {', '.join(TASKS)}\")\n",
    "        print(f\"# Model Types: baseline, anm, anm_curriculum\")\n",
    "        print(f\"# Training Steps: {TRAIN_ITERATIONS}\")\n",
    "        print(f\"# Curriculum: Percentage-based stage transitions\")\n",
    "        print(f\"{'#'*80}\\n\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        success = {}\n",
    "        model_types = ['baseline', 'anm', 'anm_curriculum']\n",
    "        \n",
    "        for dataset in TASKS:\n",
    "            for model_type in model_types:\n",
    "                key = f\"{dataset}_{model_type}\"\n",
    "                success[key] = self.train_model(dataset, model_type, force_retrain)\n",
    "        \n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"# TRAINING SUMMARY\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        for dataset in TASKS:\n",
    "            print(f\"\\n{dataset.upper()}:\")\n",
    "            for model_type in model_types:\n",
    "                key = f\"{dataset}_{model_type}\"\n",
    "                status_str = \"✓ SUCCESS\" if success.get(key, False) else \"✗ FAILED\"\n",
    "                print(f\"  {model_type:20s}: {status_str}\")\n",
    "        print(f\"{'#'*80}\\n\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        return all(success.values())\n",
    "    \n",
    "    def evaluate_all(self):\n",
    "        \"\"\"Evaluate all models on both same and harder difficulty\"\"\"\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"# EVALUATING ALL CONTINUOUS TASKS\")\n",
    "        print(f\"# Tasks: {', '.join(TASKS)}\")\n",
    "        print(f\"# Model Types: baseline, anm, anm_curriculum\")\n",
    "        print(f\"{'#'*80}\\n\")\n",
    "        sys.stdout.flush()\n",
    "        \n",
    "        results = {}\n",
    "        model_types = ['baseline', 'anm', 'anm_curriculum']\n",
    "        \n",
    "        for dataset in TASKS:\n",
    "            results[dataset] = {}\n",
    "            for model_type in model_types:\n",
    "                results[dataset][model_type] = {\n",
    "                    'same_difficulty': self.evaluate_model(dataset, model_type, ood=False),\n",
    "                    'harder_difficulty': self.evaluate_model(dataset, model_type, ood=True)\n",
    "                }\n",
    "        \n",
    "        self.results = results\n",
    "        self._print_results_table()\n",
    "        self._save_results()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _print_results_table(self):\n",
    "        \"\"\"Print results in a comparison table format\"\"\"\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"# RESULTS COMPARISON TABLE\")\n",
    "        print(f\"# Training Steps: {TRAIN_ITERATIONS}\")\n",
    "        print(f\"{'#'*80}\\n\")\n",
    "        \n",
    "        # Print header\n",
    "        print(f\"{'Task':<20s} {'Method':<25s} {'Same Difficulty':>15s} {'Harder Difficulty':>17s}\")\n",
    "        print(f\"{'-'*20} {'-'*25} {'-'*15} {'-'*17}\")\n",
    "        \n",
    "        # Task name mapping for display\n",
    "        task_display = {\n",
    "            'addition': 'Addition',\n",
    "            'lowrank': 'Matrix Completion',\n",
    "            'inverse': 'Matrix Inverse'\n",
    "        }\n",
    "        \n",
    "        # Print results for each task\n",
    "        for dataset in TASKS:\n",
    "            task_name = task_display.get(dataset, dataset)\n",
    "            \n",
    "            # Baseline\n",
    "            baseline_same = self.results.get(dataset, {}).get('baseline', {}).get('same_difficulty')\n",
    "            baseline_harder = self.results.get(dataset, {}).get('baseline', {}).get('harder_difficulty')\n",
    "            baseline_same_str = f\"{baseline_same:.4f}\" if baseline_same is not None else \"N/A\"\n",
    "            baseline_harder_str = f\"{baseline_harder:.4f}\" if baseline_harder is not None else \"N/A\"\n",
    "            print(f\"{task_name:<20s} {'IRED (baseline)':<25s} {baseline_same_str:>15s} {baseline_harder_str:>17s}\")\n",
    "            \n",
    "            # ANM without curriculum\n",
    "            anm_same = self.results.get(dataset, {}).get('anm', {}).get('same_difficulty')\n",
    "            anm_harder = self.results.get(dataset, {}).get('anm', {}).get('harder_difficulty')\n",
    "            anm_same_str = f\"{anm_same:.4f}\" if anm_same is not None else \"N/A\"\n",
    "            anm_harder_str = f\"{anm_harder:.4f}\" if anm_harder is not None else \"N/A\"\n",
    "            print(f\"{'':<20s} {'IRED + ANM':<25s} {anm_same_str:>15s} {anm_harder_str:>17s}\")\n",
    "            \n",
    "            # ANM with curriculum\n",
    "            anm_curr_same = self.results.get(dataset, {}).get('anm_curriculum', {}).get('same_difficulty')\n",
    "            anm_curr_harder = self.results.get(dataset, {}).get('anm_curriculum', {}).get('harder_difficulty')\n",
    "            anm_curr_same_str = f\"{anm_curr_same:.4f}\" if anm_curr_same is not None else \"N/A\"\n",
    "            anm_curr_harder_str = f\"{anm_curr_harder:.4f}\" if anm_curr_harder is not None else \"N/A\"\n",
    "            print(f\"{'':<20s} {'IRED + ANM + Curriculum':<25s} {anm_curr_same_str:>15s} {anm_curr_harder_str:>17s}\")\n",
    "            \n",
    "            print()  # Blank line between tasks\n",
    "        \n",
    "        # Print improvement percentages if baseline exists\n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"# RELATIVE IMPROVEMENTS vs BASELINE\")\n",
    "        print(f\"{'#'*80}\\n\")\n",
    "        \n",
    "        for dataset in TASKS:\n",
    "            task_name = task_display.get(dataset, dataset)\n",
    "            baseline_same = self.results.get(dataset, {}).get('baseline', {}).get('same_difficulty')\n",
    "            baseline_harder = self.results.get(dataset, {}).get('baseline', {}).get('harder_difficulty')\n",
    "            \n",
    "            if baseline_same and baseline_harder:\n",
    "                print(f\"{task_name}:\")\n",
    "                \n",
    "                # ANM improvements\n",
    "                anm_same = self.results.get(dataset, {}).get('anm', {}).get('same_difficulty')\n",
    "                anm_harder = self.results.get(dataset, {}).get('anm', {}).get('harder_difficulty')\n",
    "                if anm_same and anm_harder:\n",
    "                    same_imp = ((baseline_same - anm_same) / baseline_same) * 100\n",
    "                    harder_imp = ((baseline_harder - anm_harder) / baseline_harder) * 100\n",
    "                    print(f\"  ANM: {same_imp:+.1f}% (same), {harder_imp:+.1f}% (harder)\")\n",
    "                \n",
    "                # Curriculum improvements\n",
    "                curr_same = self.results.get(dataset, {}).get('anm_curriculum', {}).get('same_difficulty')\n",
    "                curr_harder = self.results.get(dataset, {}).get('anm_curriculum', {}).get('harder_difficulty')\n",
    "                if curr_same and curr_harder:\n",
    "                    same_imp = ((baseline_same - curr_same) / baseline_same) * 100\n",
    "                    harder_imp = ((baseline_harder - curr_harder) / baseline_harder) * 100\n",
    "                    print(f\"  ANM+Curriculum: {same_imp:+.1f}% (same), {harder_imp:+.1f}% (harder)\")\n",
    "        \n",
    "        print(f\"\\n{'#'*80}\")\n",
    "        print(f\"# Paper's reported IRED results for comparison:\")\n",
    "        print(f\"{'#'*80}\")\n",
    "        print(f\"{'Addition':<20s} {'IRED (paper)':<25s} {'0.0002':>15s} {'0.0020':>17s}\")\n",
    "        print(f\"{'Matrix Completion':<20s} {'IRED (paper)':<25s} {'0.0174':>15s} {'0.2054':>17s}\")\n",
    "        print(f\"{'Matrix Inverse':<20s} {'IRED (paper)':<25s} {'0.0095':>15s} {'0.2063':>17s}\")\n",
    "        print(f\"{'#'*80}\\n\")\n",
    "        sys.stdout.flush()\n",
    "    \n",
    "    def _save_results(self):\n",
    "        \"\"\"Save results to JSON file\"\"\"\n",
    "        results_file = self.base_dir / 'continuous_results_with_anm.json'\n",
    "        \n",
    "        # Add metadata\n",
    "        results_with_meta = {\n",
    "            'metadata': {\n",
    "                'batch_size': BATCH_SIZE,\n",
    "                'learning_rate': LEARNING_RATE,\n",
    "                'train_iterations': TRAIN_ITERATIONS,\n",
    "                'diffusion_steps': DIFFUSION_STEPS,\n",
    "                'rank': RANK,\n",
    "                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                'model_types': ['baseline', 'anm', 'anm_curriculum'],\n",
    "                'curriculum': {\n",
    "                    'warmup': '0-10%',\n",
    "                    'easy': '10-30%',\n",
    "                    'medium': '30-60%',\n",
    "                    'hard': '60-100%'\n",
    "                }\n",
    "            },\n",
    "            'results': self.results\n",
    "        }\n",
    "        \n",
    "        with open(results_file, 'w') as f:\n",
    "            json.dump(results_with_meta, f, indent=2)\n",
    "        \n",
    "        print(f\"Results saved to: {results_file}\\n\")\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988de22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize runner with base directory\n",
    "args = argparse.Namespace(base_dir='experiments', force=False)\n",
    "runner = ExperimentRunner(base_dir=args.base_dir)\n",
    "\n",
    "# Train all models\n",
    "success = runner.train_all(force_retrain=args.force)\n",
    "\n",
    "# Evaluate if training succeeded\n",
    "if success:\n",
    "    # Evaluate all\n",
    "    runner.evaluate_all()\n",
    "else:\n",
    "    print(\"\\nSome training jobs failed. Skipping evaluation.\")\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
