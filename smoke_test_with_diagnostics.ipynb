{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27a210",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf energy-based-model-2\n",
    "!git clone https://github.com/mdkrasnow/energy-based-model-2\n",
    "%cd energy-based-model-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9537eb64",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q torch torchvision einops accelerate tqdm tabulate matplotlib numpy pandas ema-pytorch ipdb seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb0c280",
   "metadata": {},
   "outputs": [],
   "source": "import os\nimport sys\nimport subprocess\nimport argparse\nimport json\nfrom pathlib import Path\nimport time\nimport re\nimport torch\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nfrom tqdm.notebook import tqdm\n\n# Import model components for diagnostics\nfrom diffusion_lib.denoising_diffusion_pytorch_1d import GaussianDiffusion1D\nfrom models import EBM, DiffusionWrapper\nfrom dataset import Addition, Inverse, LowRankDataset\n\n# Set style for plots\nplt.style.use('seaborn-v0_8-darkgrid')\nsns.set_palette(\"husl\")\n\n# Hyperparameters from the paper (Appendix A)\nBATCH_SIZE = 2048\nLEARNING_RATE = 1e-4\nTRAIN_ITERATIONS = 1000  \nDIFFUSION_STEPS = 10\nRANK = 20  # For 20x20 matrices\n\n# Tasks to run\nTASKS = ['addition']\n\nclass ExperimentRunner:\n    def __init__(self, base_dir='experiments'):\n        self.base_dir = Path(base_dir)\n        self.base_dir.mkdir(exist_ok=True)\n        self.results = {}\n        self.diagnostic_results = defaultdict(dict)  # Store diagnostic results\n    \n    def _sample_batch(self, dataset, batch_size):\n        \"\"\"Helper method to sample a batch from a PyTorch dataset\n        \n        Args:\n            dataset: PyTorch dataset with __getitem__ and __len__ methods\n            batch_size: Number of samples to get\n            \n        Returns:\n            Dictionary with 'x' (inputs) and 'y' (outputs) as torch tensors\n        \"\"\"\n        indices = torch.randperm(len(dataset))[:batch_size]\n        \n        x_list = []\n        y_list = []\n        \n        for idx in indices:\n            x, y = dataset[idx.item()]\n            x_list.append(torch.from_numpy(x) if isinstance(x, np.ndarray) else x)\n            y_list.append(torch.from_numpy(y) if isinstance(y, np.ndarray) else y)\n        \n        return {\n            'x': torch.stack(x_list).float(),\n            'y': torch.stack(y_list).float()\n        }\n        \n    def get_result_dir(self, dataset, model_type='baseline'):\n        \"\"\"Get the results directory for a given dataset and model type\"\"\"\n        base = f'results/ds_{dataset}/model_mlp_diffsteps_{DIFFUSION_STEPS}'\n        if model_type == 'anm':\n            base += '_anm_curriculum'  # ANM always uses curriculum\n        return base\n    \n    def load_model_for_diagnostics(self, model_dir, device='cuda'):\n        \"\"\"Load model checkpoint for diagnostic purposes with robust prefix handling\"\"\"\n        checkpoint_path = Path(model_dir) / 'model-1.pt'\n        if not checkpoint_path.exists():\n            return None\n            \n        device = device if torch.cuda.is_available() else 'cpu'\n        checkpoint = torch.load(checkpoint_path, map_location=device)\n        \n        # Get dataset dimensions\n        if 'addition' in str(model_dir):\n            dataset = Addition(\"train\", RANK, False)\n        elif 'inverse' in str(model_dir):\n            dataset = Inverse(\"train\", RANK, False)\n        else:\n            dataset = LowRankDataset(\"train\", RANK, False)\n        \n        # Initialize model (using the same model from train.py)\n        model = EBM(\n            inp_dim=dataset.inp_dim,\n            out_dim=dataset.out_dim,\n        )\n        model = DiffusionWrapper(model)\n        \n        # Load state dict - handle different checkpoint formats\n        state_dict = None\n        \n        if 'model' in checkpoint:\n            state_dict = checkpoint['model']\n        elif 'ema' in checkpoint and 'ema_model' in checkpoint['ema']:\n            # Try EMA model if available\n            state_dict = checkpoint['ema']['ema_model']\n        else:\n            # Assume the checkpoint itself is the state dict\n            state_dict = checkpoint\n        \n        # Robust prefix handling\n        if state_dict:\n            model_state = model.state_dict()\n            fixed_state_dict = {}\n            \n            # Get sample keys to detect prefix patterns\n            sample_checkpoint_key = next(iter(state_dict.keys()))\n            sample_model_key = next(iter(model_state.keys()))\n            \n            # Check for common prefixes that need removal\n            prefixes_to_remove = ['model.', 'module.']\n            prefix_removed = False\n            \n            for prefix in prefixes_to_remove:\n                if sample_checkpoint_key.startswith(prefix) and not sample_model_key.startswith(prefix):\n                    # Remove this prefix from all keys\n                    for k, v in state_dict.items():\n                        if k.startswith(prefix):\n                            new_key = k[len(prefix):]\n                            # Only include keys that exist in the target model\n                            if new_key in model_state:\n                                fixed_state_dict[new_key] = v\n                    prefix_removed = True\n                    print(f\"Info: Removed '{prefix}' prefix from checkpoint keys\")\n                    break\n            \n            if not prefix_removed:\n                # No prefix issue, but still filter out non-model parameters\n                # (like diffusion parameters that might be in the checkpoint)\n                for k, v in state_dict.items():\n                    # Skip diffusion-related parameters\n                    if any(k.startswith(skip) for skip in ['betas', 'alphas', 'sqrt', 'log', 'posterior', 'loss', 'opt_step']):\n                        continue\n                    # Only include keys that exist in the model\n                    if k in model_state:\n                        fixed_state_dict[k] = v\n            \n            # Update state_dict with the fixed version\n            state_dict = fixed_state_dict\n        \n        # Load the cleaned state dict\n        missing_keys = []\n        unexpected_keys = []\n        \n        if state_dict:\n            # Try strict loading first\n            try:\n                model.load_state_dict(state_dict, strict=True)\n            except RuntimeError as e:\n                error_msg = str(e)\n                # Parse missing and unexpected keys from error message\n                if \"Missing key(s)\" in error_msg:\n                    missing_match = re.search(r'Missing key\\(s\\) in state_dict: (.+?)(?:\\. |$)', error_msg)\n                    if missing_match:\n                        missing_keys = [k.strip().strip('\"') for k in missing_match.group(1).split(',')]\n                \n                if \"Unexpected key(s)\" in error_msg:\n                    unexpected_match = re.search(r'Unexpected key\\(s\\) in state_dict: (.+?)(?:\\. |$)', error_msg)\n                    if unexpected_match:\n                        unexpected_keys = [k.strip().strip('\"') for k in unexpected_match.group(1).split(',')]\n                \n                # Log the issue and fall back to non-strict loading\n                print(f\"Warning: Checkpoint loading issues detected:\")\n                if missing_keys:\n                    print(f\"  Missing {len(missing_keys)} keys in model\")\n                if unexpected_keys:\n                    print(f\"  Found {len(unexpected_keys)} unexpected keys in checkpoint\")\n                print(\"  Loading with strict=False to continue...\")\n                \n                # Load with strict=False as fallback\n                model.load_state_dict(state_dict, strict=False)\n        \n        model = model.to(device)\n        model.eval()\n        \n        # Setup diffusion\n        diffusion = GaussianDiffusion1D(\n            model,\n            seq_length=32,\n            objective='pred_noise',\n            timesteps=DIFFUSION_STEPS,\n            sampling_timesteps=DIFFUSION_STEPS,\n            continuous=True,\n            show_inference_tqdm=False\n        )\n        \n        return model, diffusion, device, dataset\n    \n    def run_energy_diagnostics(self, model_dir, dataset='addition', num_batches=5):\n        \"\"\"Run energy distribution diagnostics on a trained model\"\"\"\n        result = self.load_model_for_diagnostics(model_dir)\n        if result is None:\n            return None\n            \n        model, diffusion, device, data = result\n        \n        energies = {\n            'clean': [],\n            'ired_standard': [],\n            'anm_adversarial': [],\n            'gaussian_noise': []\n        }\n        \n        print(f\"  Running energy distribution analysis...\")\n        \n        for _ in range(num_batches):\n            # Get test batch - properly split into input and output\n            batch = self._sample_batch(data, 256)\n            x_clean = batch['x'].to(device)\n            y_clean = batch['y'].to(device)\n            \n            t = torch.randint(0, diffusion.num_timesteps, (len(x_clean),), device=device)\n            \n            # 1. Clean samples energy\n            with torch.no_grad():\n                energy_clean = diffusion.energy_score(x_clean, y_clean, t)\n                energies['clean'].append(energy_clean.mean().item())\n            \n            # 2. Standard IRED corruption (corrupt the output y, not input x)\n            noise = torch.randn_like(y_clean)\n            alpha = 1.0 - (t.float() / diffusion.num_timesteps).view(-1, 1)\n            y_ired = alpha * y_clean + (1 - alpha) * noise\n            with torch.no_grad():\n                energy_ired = diffusion.energy_score(x_clean, y_ired, t)\n                energies['ired_standard'].append(energy_ired.mean().item())\n            \n            # 3. ANM adversarial corruption (simulated on output)\n            y_anm = self._simulate_anm_output(x_clean, y_clean.clone(), t, diffusion, num_steps=5)\n            with torch.no_grad():\n                energy_anm = diffusion.energy_score(x_clean, y_anm, t)\n                energies['anm_adversarial'].append(energy_anm.mean().item())\n            \n            # 4. Gaussian noise corruption (on output)\n            y_gaussian = y_clean + 0.1 * torch.randn_like(y_clean)\n            with torch.no_grad():\n                energy_gaussian = diffusion.energy_score(x_clean, y_gaussian, t)\n                energies['gaussian_noise'].append(energy_gaussian.mean().item())\n        \n        return energies\n    \n    def _simulate_anm(self, x, t, diffusion, num_steps=5, eps=0.1):\n        \"\"\"Simulate ANM adversarial corruption (deprecated - use _simulate_anm_output)\"\"\"\n        x_adv = x.clone().requires_grad_(True)\n        \n        for _ in range(num_steps):\n            energy = diffusion.energy_score(x, x_adv, t)\n            grad = torch.autograd.grad(energy.sum(), x_adv)[0]\n            \n            with torch.no_grad():\n                x_adv = x_adv + eps * grad.sign()\n                distance_penalty = 0.1\n                x_adv = x + distance_penalty * (x_adv - x)\n            \n            x_adv.requires_grad_(True)\n        \n        return x_adv.detach()\n    \n    def _simulate_anm_output(self, x_clean, y_clean, t, diffusion, num_steps=5, eps=0.1):\n        \"\"\"Simulate ANM adversarial corruption on output y given input x\"\"\"\n        y_adv = y_clean.clone().requires_grad_(True)\n        \n        for _ in range(num_steps):\n            energy = diffusion.energy_score(x_clean, y_adv, t)\n            grad = torch.autograd.grad(energy.sum(), y_adv)[0]\n            \n            with torch.no_grad():\n                y_adv = y_adv + eps * grad.sign()\n                distance_penalty = 0.1\n                y_adv = y_clean + distance_penalty * (y_adv - y_clean)\n            \n            y_adv.requires_grad_(True)\n        \n        return y_adv.detach()\n    \n    def run_comparative_diagnostics(self, baseline_dir, anm_dir, dataset='addition'):\n        \"\"\"Critical test: Direct comparison on same batch\"\"\"\n        baseline_result = self.load_model_for_diagnostics(baseline_dir)\n        anm_result = self.load_model_for_diagnostics(anm_dir)\n        \n        if baseline_result is None or anm_result is None:\n            return None\n            \n        _, baseline_diffusion, device, data = baseline_result\n        _, anm_diffusion, _, _ = anm_result\n        \n        print(f\"  Running comparative analysis...\")\n        \n        # Get test batch - properly split into input and output\n        batch = self._sample_batch(data, 100)\n        x_clean = batch['x'].to(device)\n        y_clean = batch['y'].to(device)\n        \n        t = torch.randint(0, baseline_diffusion.num_timesteps, (len(x_clean),), device=device)\n        \n        # Generate negatives using both methods (on output y)\n        noise = torch.randn_like(y_clean)\n        alpha = 1.0 - (t.float() / baseline_diffusion.num_timesteps).view(-1, 1)\n        y_ired = alpha * y_clean + (1 - alpha) * noise\n        y_anm = self._simulate_anm_output(x_clean, y_clean.clone(), t, anm_diffusion, num_steps=10)\n        \n        # Compute energies\n        with torch.no_grad():\n            energy_ired_baseline = baseline_diffusion.energy_score(x_clean, y_ired, t).mean().item()\n            energy_anm_model = anm_diffusion.energy_score(x_clean, y_anm, t).mean().item()\n        \n        # Compute distances\n        dist_ired = F.mse_loss(y_ired, y_clean).item()\n        dist_anm = F.mse_loss(y_anm, y_clean).item()\n        \n        return {\n            'energy_ired': energy_ired_baseline,\n            'energy_anm': energy_anm_model,\n            'distance_ired': dist_ired,\n            'distance_anm': dist_anm,\n            'energy_ratio': energy_anm_model / (energy_ired_baseline + 1e-8)\n        }\n    \n    def train_model(self, dataset, model_type='baseline', force_retrain=False):\n        \"\"\"Train a model for a specific dataset and model type\n        \n        Args:\n            dataset: Dataset name\n            model_type: One of 'baseline', 'anm' (which always uses curriculum)\n            force_retrain: Force retraining even if model exists\n        \"\"\"\n        result_dir = self.get_result_dir(dataset, model_type)\n        \n        # Check if model already exists\n        if not force_retrain and os.path.exists(f'{result_dir}/model-1.pt'):\n            print(f\"\\n{'='*80}\")\n            print(f\"Model for {dataset} ({model_type}) already exists. Skipping training.\")\n            print(f\"Use --force to retrain.\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            \n            # Run diagnostics on existing model\n            print(f\"Running diagnostics on existing {model_type} model...\")\n            energies = self.run_energy_diagnostics(result_dir, dataset)\n            if energies:\n                self.diagnostic_results[f'{dataset}_{model_type}']['energies'] = energies\n                self._print_energy_summary(energies, model_type)\n            \n            return True\n            \n        print(f\"\\n{'='*80}\")\n        print(f\"Training IRED ({model_type.upper()}) on {dataset.upper()} task\")\n        print(f\"{'='*80}\")\n        print(f\"Model Type: {model_type}\")\n        print(f\"Batch size: {BATCH_SIZE}\")\n        print(f\"Learning rate: {LEARNING_RATE}\")\n        print(f\"Training iterations: {TRAIN_ITERATIONS}\")\n        print(f\"Diffusion steps: {DIFFUSION_STEPS}\")\n        print(f\"Matrix rank: {RANK}\")\n        print(f\"Result directory: {result_dir}\")\n        \n        if model_type == 'anm':\n            print(f\"\\nANM with AGGRESSIVE Curriculum Schedule (% of {TRAIN_ITERATIONS} steps):\")\n            print(f\"  Warmup (0-10%): 100% clean, 0% adversarial, ε=0.0\")\n            print(f\"  Rapid Introduction (10-25%): 50% clean, 40% adversarial, 10% gaussian, ε=0.3\")\n            print(f\"  Aggressive Ramp (25-50%): 20% clean, 70% adversarial, 10% gaussian, ε=0.7\")\n            print(f\"  High Intensity (50-80%): 10% clean, 85% adversarial, 5% gaussian, ε=1.0\")\n            print(f\"  Extreme Hardening (80-100%): 5% clean, 90% adversarial, 5% gaussian, ε=1.2\")\n            \n        print(f\"{'='*80}\\n\")\n        sys.stdout.flush()\n        \n        # Build command\n        cmd = [\n            'python', 'train.py',\n            '--dataset', dataset,\n            '--model', 'mlp',\n            '--batch_size', str(BATCH_SIZE),\n            '--diffusion_steps', str(DIFFUSION_STEPS),\n            '--rank', str(RANK),\n            '--train-steps', str(TRAIN_ITERATIONS),  # Pass training steps\n        ]\n        \n        # Add model-specific parameters\n        if model_type == 'anm':\n            cmd.extend([\n                '--use-anm',\n                '--anm-adversarial-steps', '5',\n                '--anm-distance-penalty', '0.1',\n                # ANM now always uses curriculum, no need for --use-curriculum flag\n            ])\n        \n        # Run training with real-time output\n        try:\n            start_time = time.time()\n            \n            # Use subprocess.Popen for real-time output with flushing\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n                bufsize=1\n            )\n            \n            # Display output line by line as it comes\n            for line in iter(process.stdout.readline, ''):\n                if line:\n                    print(line.rstrip())\n                    sys.stdout.flush()\n            \n            # Wait for process to complete\n            result = process.wait()\n            elapsed = time.time() - start_time\n            \n            if result == 0:\n                print(f\"\\n{'='*80}\")\n                print(f\"Training completed for {dataset} ({model_type}) in {elapsed/60:.2f} minutes\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                \n                # Run diagnostics immediately after training\n                print(f\"Running diagnostics on newly trained {model_type} model...\")\n                energies = self.run_energy_diagnostics(result_dir, dataset)\n                if energies:\n                    self.diagnostic_results[f'{dataset}_{model_type}']['energies'] = energies\n                    self._print_energy_summary(energies, model_type)\n                \n                return True\n            else:\n                print(f\"\\n{'='*80}\")\n                print(f\"ERROR: Training failed for {dataset} ({model_type}) with exit code {result}\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                return False\n            \n        except Exception as e:\n            print(f\"\\n{'='*80}\")\n            print(f\"ERROR: Training failed for {dataset} ({model_type}): {e}\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            return False\n    \n    def _print_energy_summary(self, energies, model_type):\n        \"\"\"Print energy distribution summary\"\"\"\n        print(f\"\\n  Energy Statistics for {model_type.upper()} model:\")\n        print(\"  \" + \"-\" * 50)\n        for corruption_type, values in energies.items():\n            mean_energy = np.mean(values)\n            std_energy = np.std(values)\n            print(f\"  {corruption_type:20s}: {mean_energy:.4f} ± {std_energy:.4f}\")\n        \n        # Key insight for ANM\n        if model_type == 'anm':\n            mean_ired = np.mean(energies['ired_standard'])\n            mean_anm = np.mean(energies['anm_adversarial'])\n            improvement = ((mean_anm - mean_ired) / abs(mean_ired)) * 100\n            \n            print(\"\\n  \" + \"=\"*50)\n            print(\"  ANM DIAGNOSTIC:\")\n            if abs(improvement) < 5:\n                print(\"  ❌ ANM energies ≈ IRED energies → ANM is REDUNDANT\")\n            elif improvement < -10:\n                print(\"  ⚠️  ANM energies < IRED energies → ANM is TOO WEAK\")\n            elif improvement > 50:\n                print(\"  ⚠️  ANM energies >> IRED energies → ANM may be OFF-MANIFOLD\")\n            else:\n                print(f\"  ✓ ANM provides {improvement:.1f}% energy increase over IRED\")\n            print(\"  \" + \"=\"*50 + \"\\n\")\n    \n    def evaluate_model(self, dataset, model_type='baseline', ood=False):\n        \"\"\"Evaluate a trained model on same or harder difficulty\"\"\"\n        result_dir = self.get_result_dir(dataset, model_type)\n        \n        # Check if model exists\n        if not os.path.exists(f'{result_dir}/model-1.pt'):\n            print(f\"\\n{'='*80}\")\n            print(f\"ERROR: No trained model found for {dataset} ({model_type})\")\n            print(f\"Expected location: {result_dir}/model-1.pt\")\n            print(f\"Please train the model first.\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            return None\n        \n        difficulty = \"Harder Difficulty (OOD)\" if ood else \"Same Difficulty\"\n        print(f\"\\n{'='*80}\")\n        print(f\"Evaluating IRED ({model_type.upper()}) on {dataset.upper()} - {difficulty}\")\n        print(f\"{'='*80}\\n\")\n        sys.stdout.flush()\n        \n        # Build command\n        cmd = [\n            'python', 'train.py',\n            '--dataset', dataset,\n            '--model', 'mlp',\n            '--batch_size', str(BATCH_SIZE),\n            '--diffusion_steps', str(DIFFUSION_STEPS),\n            '--rank', str(RANK),\n            '--train-steps', str(TRAIN_ITERATIONS),  # Pass for consistency\n            '--load-milestone', '1',\n            '--evaluate',\n        ]\n        \n        # Add model-specific parameters for evaluation\n        if model_type == 'anm':\n            cmd.extend([\n                '--use-anm',\n                '--anm-adversarial-steps', '5',\n                '--anm-distance-penalty', '0.1',\n                # ANM now always uses curriculum, no need for --use-curriculum flag\n            ])\n        \n        if ood:\n            cmd.append('--ood')\n        \n        # Run evaluation with real-time output\n        try:\n            # Collect output for MSE parsing while also displaying it\n            output_lines = []\n            \n            # Use subprocess.Popen for real-time output with flushing\n            process = subprocess.Popen(\n                cmd,\n                stdout=subprocess.PIPE,\n                stderr=subprocess.STDOUT,\n                universal_newlines=True,\n                bufsize=1\n            )\n            \n            # Display output line by line as it comes\n            for line in iter(process.stdout.readline, ''):\n                if line:\n                    print(line.rstrip())\n                    sys.stdout.flush()\n                    output_lines.append(line)\n            \n            # Wait for process to complete\n            result = process.wait()\n            \n            if result == 0:\n                # Parse output to extract MSE\n                output_text = ''.join(output_lines)\n                mse = self._parse_mse_from_output(output_text, '')\n                \n                print(f\"\\n{'='*80}\")\n                print(f\"Evaluation completed for {dataset} ({model_type}) - {difficulty}\")\n                if mse is not None:\n                    print(f\"MSE: {mse:.4f}\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                \n                return mse\n            else:\n                print(f\"\\n{'='*80}\")\n                print(f\"ERROR: Evaluation failed for {dataset} ({model_type}) - {difficulty} with exit code {result}\")\n                print(f\"{'='*80}\\n\")\n                sys.stdout.flush()\n                return None\n            \n        except Exception as e:\n            print(f\"\\n{'='*80}\")\n            print(f\"ERROR: Evaluation failed for {dataset} ({model_type}) - {difficulty}: {e}\")\n            print(f\"{'='*80}\\n\")\n            sys.stdout.flush()\n            return None\n    \n    def _parse_mse_from_output(self, stdout, stderr):\n        \"\"\"Parse MSE from training/evaluation output\"\"\"\n        output = stdout + stderr\n        lines = output.split('\\n')\n        \n        # Look for validation result tables with MSE values\n        mse_value = None\n        for i, line in enumerate(lines):\n            # Look for the specific pattern of mse in a table\n            if line.startswith('mse') and '  ' in line:\n                # This looks like a table row with MSE\n                parts = line.split()\n                if len(parts) >= 2 and parts[0] == 'mse':\n                    try:\n                        mse_value = float(parts[1])\n                        # Continue searching to find the last MSE value (most recent)\n                    except (ValueError, IndexError):\n                        pass\n        \n        # If we didn't find MSE in table format, try alternative formats\n        if mse_value is None:\n            # Look for patterns like \"mse_error  0.635722\"\n            for line in lines:\n                if 'mse_error' in line.lower():\n                    parts = line.split()\n                    for i, part in enumerate(parts):\n                        if 'mse' in part.lower() and i + 1 < len(parts):\n                            try:\n                                mse_value = float(parts[i + 1])\n                            except ValueError:\n                                pass\n        \n        return mse_value\n    \n    def train_all(self, force_retrain=False):\n        \"\"\"Train all models (baseline and ANM with curriculum)\"\"\"\n        print(f\"\\n{'#'*80}\")\n        print(f\"# TRAINING ALL CONTINUOUS TASKS WITH DIAGNOSTICS\")\n        print(f\"# Tasks: {', '.join(TASKS)}\")\n        print(f\"# Model Types: baseline, anm (with curriculum)\")\n        print(f\"# Training Steps: {TRAIN_ITERATIONS}\")\n        print(f\"# ANM uses AGGRESSIVE curriculum (predefined from curriculum_config.py)\")\n        print(f\"# Diagnostics will run automatically after each training\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n        \n        success = {}\n        model_types = ['baseline', 'anm']\n        \n        for dataset in TASKS:\n            for model_type in model_types:\n                key = f\"{dataset}_{model_type}\"\n                success[key] = self.train_model(dataset, model_type, force_retrain)\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# TRAINING SUMMARY\")\n        print(f\"{'#'*80}\")\n        for dataset in TASKS:\n            print(f\"\\n{dataset.upper()}:\")\n            for model_type in model_types:\n                key = f\"{dataset}_{model_type}\"\n                status_str = \"✓ SUCCESS\" if success.get(key, False) else \"✗ FAILED\"\n                model_desc = \"ANM+Curriculum\" if model_type == 'anm' else model_type\n                print(f\"  {model_desc:20s}: {status_str}\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n        \n        return all(success.values())\n    \n    def evaluate_all(self):\n        \"\"\"Evaluate all models on both same and harder difficulty\"\"\"\n        print(f\"\\n{'#'*80}\")\n        print(f\"# EVALUATING ALL CONTINUOUS TASKS\")\n        print(f\"# Tasks: {', '.join(TASKS)}\")\n        print(f\"# Model Types: baseline, anm (with curriculum)\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n        \n        results = {}\n        model_types = ['baseline', 'anm']\n        \n        for dataset in TASKS:\n            results[dataset] = {}\n            for model_type in model_types:\n                results[dataset][model_type] = {\n                    'same_difficulty': self.evaluate_model(dataset, model_type, ood=False),\n                    'harder_difficulty': self.evaluate_model(dataset, model_type, ood=True)\n                }\n        \n        self.results = results\n        \n        # Run comparative diagnostics after evaluation\n        print(f\"\\n{'#'*80}\")\n        print(f\"# RUNNING COMPARATIVE DIAGNOSTICS\")\n        print(f\"{'#'*80}\\n\")\n        \n        for dataset in TASKS:\n            baseline_dir = self.get_result_dir(dataset, 'baseline')\n            anm_dir = self.get_result_dir(dataset, 'anm')\n            \n            comp_result = self.run_comparative_diagnostics(baseline_dir, anm_dir, dataset)\n            if comp_result:\n                self.diagnostic_results[f'{dataset}_comparative'] = comp_result\n        \n        self._print_results_table()\n        self._save_results()\n        \n        return results\n    \n    def _print_results_table(self):\n        \"\"\"Print results in a comparison table format\"\"\"\n        print(f\"\\n{'#'*80}\")\n        print(f\"# RESULTS COMPARISON TABLE\")\n        print(f\"# Training Steps: {TRAIN_ITERATIONS}\")\n        print(f\"{'#'*80}\\n\")\n        \n        # Print header\n        print(f\"{'Task':<20s} {'Method':<25s} {'Same Difficulty':>15s} {'Harder Difficulty':>17s}\")\n        print(f\"{'-'*20} {'-'*25} {'-'*15} {'-'*17}\")\n        \n        # Task name mapping for display\n        task_display = {\n            'addition': 'Addition',\n            'lowrank': 'Matrix Completion',\n            'inverse': 'Matrix Inverse'\n        }\n        \n        # Print results for each task\n        for dataset in TASKS:\n            task_name = task_display.get(dataset, dataset)\n            \n            # Baseline\n            baseline_same = self.results.get(dataset, {}).get('baseline', {}).get('same_difficulty')\n            baseline_harder = self.results.get(dataset, {}).get('baseline', {}).get('harder_difficulty')\n            baseline_same_str = f\"{baseline_same:.4f}\" if baseline_same is not None else \"N/A\"\n            baseline_harder_str = f\"{baseline_harder:.4f}\" if baseline_harder is not None else \"N/A\"\n            print(f\"{task_name:<20s} {'IRED (baseline)':<25s} {baseline_same_str:>15s} {baseline_harder_str:>17s}\")\n            \n            # ANM with curriculum\n            anm_same = self.results.get(dataset, {}).get('anm', {}).get('same_difficulty')\n            anm_harder = self.results.get(dataset, {}).get('anm', {}).get('harder_difficulty')\n            anm_same_str = f\"{anm_same:.4f}\" if anm_same is not None else \"N/A\"\n            anm_harder_str = f\"{anm_harder:.4f}\" if anm_harder is not None else \"N/A\"\n            print(f\"{'':<20s} {'IRED + ANM (curriculum)':<25s} {anm_same_str:>15s} {anm_harder_str:>17s}\")\n            \n            print()  # Blank line between tasks\n        \n        # Print improvement percentages if baseline exists\n        print(f\"\\n{'#'*80}\")\n        print(f\"# RELATIVE IMPROVEMENTS vs BASELINE\")\n        print(f\"{'#'*80}\\n\")\n        \n        for dataset in TASKS:\n            task_name = task_display.get(dataset, dataset)\n            baseline_same = self.results.get(dataset, {}).get('baseline', {}).get('same_difficulty')\n            baseline_harder = self.results.get(dataset, {}).get('baseline', {}).get('harder_difficulty')\n            \n            if baseline_same and baseline_harder:\n                print(f\"{task_name}:\")\n                \n                # ANM improvements\n                anm_same = self.results.get(dataset, {}).get('anm', {}).get('same_difficulty')\n                anm_harder = self.results.get(dataset, {}).get('anm', {}).get('harder_difficulty')\n                if anm_same and anm_harder:\n                    same_imp = ((baseline_same - anm_same) / baseline_same) * 100\n                    harder_imp = ((baseline_harder - anm_harder) / baseline_harder) * 100\n                    print(f\"  ANM+Curriculum: {same_imp:+.1f}% (same), {harder_imp:+.1f}% (harder)\")\n        \n        print(f\"\\n{'#'*80}\")\n        print(f\"# Paper's reported IRED results for comparison:\")\n        print(f\"{'#'*80}\")\n        print(f\"{'Addition':<20s} {'IRED (paper)':<25s} {'0.0002':>15s} {'0.0020':>17s}\")\n        print(f\"{'Matrix Completion':<20s} {'IRED (paper)':<25s} {'0.0174':>15s} {'0.2054':>17s}\")\n        print(f\"{'Matrix Inverse':<20s} {'IRED (paper)':<25s} {'0.0095':>15s} {'0.2063':>17s}\")\n        print(f\"{'#'*80}\\n\")\n        sys.stdout.flush()\n    \n    def _save_results(self):\n        \"\"\"Save results to JSON file\"\"\"\n        results_file = self.base_dir / 'continuous_results_with_anm_diagnostics.json'\n        \n        # Add metadata\n        results_with_meta = {\n            'metadata': {\n                'batch_size': BATCH_SIZE,\n                'learning_rate': LEARNING_RATE,\n                'train_iterations': TRAIN_ITERATIONS,\n                'diffusion_steps': DIFFUSION_STEPS,\n                'rank': RANK,\n                'timestamp': time.strftime('%Y-%m-%d %H:%M:%S'),\n                'model_types': ['baseline', 'anm'],\n                'anm_note': 'ANM always uses curriculum learning',\n                'curriculum': {\n                    'type': 'AGGRESSIVE_CURRICULUM',\n                    'warmup': '0-10% (100% clean, ε=0.0)',\n                    'rapid_introduction': '10-25% (50% clean, 40% adversarial, ε=0.3)',\n                    'aggressive_ramp': '25-50% (20% clean, 70% adversarial, ε=0.7)',\n                    'high_intensity': '50-80% (10% clean, 85% adversarial, ε=1.0)',\n                    'extreme_hardening': '80-100% (5% clean, 90% adversarial, ε=1.2)'\n                }\n            },\n            'results': self.results,\n            'diagnostics': dict(self.diagnostic_results)  # Include diagnostic results\n        }\n        \n        with open(results_file, 'w') as f:\n            json.dump(results_with_meta, f, indent=2)\n        \n        print(f\"Results saved to: {results_file}\\n\")\n        sys.stdout.flush()\n    \n    def plot_diagnostic_results(self):\n        \"\"\"Create inline visualizations of diagnostic results\"\"\"\n        if not self.diagnostic_results:\n            print(\"No diagnostic results to plot.\")\n            return\n        \n        # Create figure with subplots\n        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n        fig.suptitle('ANM Diagnostic Analysis', fontsize=16, y=1.02)\n        \n        # Plot 1: Energy distributions for baseline and ANM\n        ax = axes[0, 0]\n        for model_type in ['baseline', 'anm']:\n            key = f'addition_{model_type}'\n            if key in self.diagnostic_results and 'energies' in self.diagnostic_results[key]:\n                energies = self.diagnostic_results[key]['energies']\n                x = ['Clean', 'IRED', 'ANM', 'Gaussian']\n                y = [np.mean(energies[k]) for k in ['clean', 'ired_standard', 'anm_adversarial', 'gaussian_noise']]\n                label = 'ANM Model' if model_type == 'anm' else 'Baseline Model'\n                ax.plot(x, y, marker='o', label=label, linewidth=2)\n        \n        ax.set_ylabel('Mean Energy')\n        ax.set_title('Energy Distribution Comparison')\n        ax.legend()\n        ax.grid(True, alpha=0.3)\n        \n        # Plot 2: Comparative analysis\n        ax = axes[0, 1]\n        comp_key = 'addition_comparative'\n        if comp_key in self.diagnostic_results:\n            comp = self.diagnostic_results[comp_key]\n            methods = ['IRED\\nStandard', 'ANM\\nAdversarial']\n            energies = [comp['energy_ired'], comp['energy_anm']]\n            colors = ['blue', 'red']\n            bars = ax.bar(methods, energies, color=colors, alpha=0.7)\n            \n            # Add value labels\n            for bar, val in zip(bars, energies):\n                ax.text(bar.get_x() + bar.get_width()/2, val + 0.001,\n                        f'{val:.4f}', ha='center', va='bottom')\n            \n            ax.set_ylabel('Energy Score')\n            ax.set_title('Direct Energy Comparison')\n            ax.grid(True, alpha=0.3)\n        \n        # Plot 3: Energy ratio and verdict\n        ax = axes[0, 2]\n        ax.axis('off')\n        \n        if comp_key in self.diagnostic_results:\n            comp = self.diagnostic_results[comp_key]\n            ratio = comp['energy_ratio']\n            improvement = (ratio - 1.0) * 100\n            \n            verdict_text = \"📊 CRITICAL DIAGNOSTIC\\n\\n\"\n            if 0.95 <= ratio <= 1.05:\n                verdict_text += \"❌ ANM ≈ IRED\\n\\nANM is REDUNDANT!\\n\\n\"\n                verdict_text += \"ANM finds same negatives\\nas standard IRED.\"\n                color = 'red'\n            elif ratio < 0.95:\n                verdict_text += \"❌ ANM < IRED\\n\\nANM is HARMFUL!\\n\\n\"\n                verdict_text += f\"ANM reduces energy by\\n{-improvement:.1f}%\"\n                color = 'darkred'\n            else:\n                verdict_text += f\"✓ ANM > IRED\\n\\n{improvement:.1f}% improvement\\n\\n\"\n                verdict_text += \"ANM successfully finds\\nharder negatives.\"\n                color = 'green'\n            \n            verdict_text += f\"\\n\\nEnergy Ratio: {ratio:.3f}\"\n            \n            ax.text(0.5, 0.5, verdict_text, transform=ax.transAxes,\n                    fontsize=12, ha='center', va='center',\n                    bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8),\n                    color=color, weight='bold')\n        \n        # Plot 4: Energy difference breakdown\n        ax = axes[1, 0]\n        if 'addition_anm' in self.diagnostic_results and 'energies' in self.diagnostic_results['addition_anm']:\n            energies = self.diagnostic_results['addition_anm']['energies']\n            \n            # Calculate differences from IRED standard\n            ired_mean = np.mean(energies['ired_standard'])\n            diffs = {\n                'Clean': np.mean(energies['clean']) - ired_mean,\n                'ANM': np.mean(energies['anm_adversarial']) - ired_mean,\n                'Gaussian': np.mean(energies['gaussian_noise']) - ired_mean\n            }\n            \n            colors = ['green' if d < 0 else 'red' for d in diffs.values()]\n            bars = ax.bar(diffs.keys(), diffs.values(), color=colors, alpha=0.7)\n            ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n            ax.set_ylabel('Energy Difference from IRED')\n            ax.set_title('Energy Differences (ANM Model)')\n            ax.grid(True, alpha=0.3)\n        \n        # Plot 5: Distance comparison\n        ax = axes[1, 1]\n        if comp_key in self.diagnostic_results:\n            comp = self.diagnostic_results[comp_key]\n            methods = ['IRED', 'ANM']\n            distances = [comp['distance_ired'], comp['distance_anm']]\n            colors = ['blue', 'red']\n            bars = ax.bar(methods, distances, color=colors, alpha=0.7)\n            \n            for bar, val in zip(bars, distances):\n                ax.text(bar.get_x() + bar.get_width()/2, val + 0.0001,\n                        f'{val:.4f}', ha='center', va='bottom')\n            \n            ax.set_ylabel('L2 Distance from Original')\n            ax.set_title('Sample Movement Distance')\n            ax.grid(True, alpha=0.3)\n        \n        # Plot 6: Recommendations\n        ax = axes[1, 2]\n        ax.axis('off')\n        \n        recommendations = \"📋 RECOMMENDATIONS\\n\\n\"\n        if comp_key in self.diagnostic_results:\n            ratio = self.diagnostic_results[comp_key]['energy_ratio']\n            \n            if 0.95 <= ratio <= 1.05:\n                recommendations += \"1. Increase --anm-adversarial-steps\\n   from 5 to 20-50\\n\\n\"\n                recommendations += \"2. Increase epsilon to 0.5-1.0\\n\\n\"\n                recommendations += \"3. Reduce distance penalty\\n   from 0.1 to 0.01\\n\\n\"\n                recommendations += \"4. Start ANM earlier\\n   (5% instead of 10%)\"\n            elif ratio < 0.95:\n                recommendations += \"1. Check gradient sign\\n   (should maximize energy)\\n\\n\"\n                recommendations += \"2. Verify energy computation\\n\\n\"\n                recommendations += \"3. Ensure model.eval() before ANM\\n\\n\"\n                recommendations += \"4. Check for gradient clipping\"\n            else:\n                recommendations += \"✓ Current settings working\\n\\n\"\n                recommendations += \"Consider:\\n\"\n                recommendations += \"• Longer training\\n\"\n                recommendations += \"• Fine-tune epsilon\\n\"\n                recommendations += \"• Adjust adversarial steps\"\n        \n        ax.text(0.1, 0.9, recommendations, transform=ax.transAxes,\n                fontsize=10, verticalalignment='top',\n                bbox=dict(boxstyle='round', facecolor='lightblue', alpha=0.5))\n        \n        plt.tight_layout()\n        plt.show()\n    \n    def print_diagnostic_summary(self):\n        \"\"\"Print comprehensive diagnostic summary with recommendations\"\"\"\n        print(f\"\\n{'='*80}\")\n        print(\"COMPREHENSIVE DIAGNOSTIC SUMMARY\")\n        print(\"=\"*80)\n        \n        # Check if we have comparative results\n        comp_key = 'addition_comparative'\n        if comp_key in self.diagnostic_results:\n            comp = self.diagnostic_results[comp_key]\n            ratio = comp['energy_ratio']\n            \n            print(\"\\n📊 FINAL VERDICT:\")\n            if 0.95 <= ratio <= 1.05:\n                print(\"  ❌ ANM is REDUNDANT - provides no improvement over standard IRED\\n\")\n                print(\"  CRITICAL FINDING: ANM adversarial samples have nearly identical\")\n                print(\"  energy to standard IRED negatives, making the additional computation\")\n                print(\"  unnecessary.\\n\")\n                \n                print(\"  RECOMMENDED FIXES:\")\n                print(\"  1. Increase anm_adversarial_steps from 5 to 20-50\")\n                print(\"  2. Increase epsilon from 0.1 to 0.5-1.0\")\n                print(\"  3. Reduce anm_distance_penalty from 0.1 to 0.01\")\n                print(\"  4. Start ANM earlier in curriculum (5% instead of 10%)\")\n                print(\"  5. Use more aggressive curriculum with higher ANM percentages\")\n                \n                print(\"\\n  CODE CHANGES NEEDED:\")\n                print(\"  - In train.py: --anm-adversarial-steps 20\")\n                print(\"  - In adversarial_corruption.py: increase eps_iter\")\n                print(\"  - In denoising_diffusion_pytorch_1d.py: reduce distance_penalty weight\")\n                \n            elif ratio < 0.95:\n                print(\"  ❌ ANM is HARMFUL - actually degrading performance\\n\")\n                degradation = (1.0 - ratio) * 100\n                print(f\"  CRITICAL FINDING: ANM is reducing energy by {degradation:.1f}%,\")\n                print(\"  making negatives EASIER instead of harder!\\n\")\n                \n                print(\"  RECOMMENDED FIXES:\")\n                print(\"  1. Check gradient sign (should maximize energy, not minimize)\")\n                print(\"  2. Verify energy function computation\")\n                print(\"  3. Check for gradient clipping issues\")\n                print(\"  4. Ensure model is in eval mode during ANM generation\")\n                \n                print(\"\\n  CODE CHANGES NEEDED:\")\n                print(\"  - In adversarial_corruption.py: verify gradient sign\")\n                print(\"  - Check model.eval() is called before ANM\")\n                print(\"  - Review energy_score implementation\")\n                \n            else:\n                improvement = (ratio - 1) * 100\n                print(f\"  ✓ ANM provides {improvement:.1f}% improvement\\n\")\n                print(\"  SUCCESS: ANM is successfully finding harder negatives than IRED.\\n\")\n                \n                print(\"  OPTIMIZATION SUGGESTIONS:\")\n                print(\"  1. Current settings are working\")\n                print(\"  2. Could try increasing training steps\")\n                print(\"  3. Fine-tune epsilon and adversarial steps\")\n                print(\"  4. Consider more aggressive curriculum\")\n        \n        # Print energy analysis\n        if 'addition_anm' in self.diagnostic_results and 'energies' in self.diagnostic_results['addition_anm']:\n            energies = self.diagnostic_results['addition_anm']['energies']\n            \n            print(\"\\n📈 ENERGY LANDSCAPE ANALYSIS:\")\n            print(\"  \" + \"-\"*50)\n            \n            mean_ired = np.mean(energies['ired_standard'])\n            mean_anm = np.mean(energies['anm_adversarial'])\n            mean_gaussian = np.mean(energies['gaussian_noise'])\n            mean_clean = np.mean(energies['clean'])\n            \n            print(f\"  Clean samples:     {mean_clean:.4f} (baseline)\")\n            print(f\"  Gaussian noise:    {mean_gaussian:.4f} ({(mean_gaussian/mean_clean-1)*100:+.1f}% vs clean)\")\n            print(f\"  IRED standard:     {mean_ired:.4f} ({(mean_ired/mean_clean-1)*100:+.1f}% vs clean)\")\n            print(f\"  ANM adversarial:   {mean_anm:.4f} ({(mean_anm/mean_clean-1)*100:+.1f}% vs clean)\")\n            \n            print(\"\\n  INTERPRETATION:\")\n            if mean_anm < mean_gaussian:\n                print(\"  ⚠️  ANM samples have LOWER energy than random noise!\")\n                print(\"     This indicates a serious problem with the adversarial process.\")\n            elif mean_anm < mean_ired * 1.05:\n                print(\"  ⚠️  ANM barely improves over standard IRED.\")\n                print(\"     The adversarial process needs stronger hyperparameters.\")\n            else:\n                print(\"  ✓  ANM successfully creates harder negatives than IRED.\")\n        \n        print(\"\\n\" + \"=\"*80)\n        print(\"END OF DIAGNOSTIC SUMMARY\")\n        print(\"=\"*80 + \"\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988de22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize runner with base directory\n",
    "args = argparse.Namespace(base_dir='experiments', force=False)\n",
    "runner = ExperimentRunner(base_dir=args.base_dir)\n",
    "\n",
    "# Train all models (diagnostics run automatically after each training)\n",
    "success = runner.train_all(force_retrain=args.force)\n",
    "\n",
    "# Evaluate if training succeeded\n",
    "if success:\n",
    "    # Evaluate all models (comparative diagnostics run automatically)\n",
    "    results = runner.evaluate_all()\n",
    "    \n",
    "    # Plot diagnostic visualizations\n",
    "    print(\"\\n\" + \"#\"*80)\n",
    "    print(\"# DIAGNOSTIC VISUALIZATIONS\")\n",
    "    print(\"#\"*80)\n",
    "    runner.plot_diagnostic_results()\n",
    "    \n",
    "    # Print comprehensive diagnostic summary with recommendations\n",
    "    runner.print_diagnostic_summary()\n",
    "else:\n",
    "    print(\"\\nSome training jobs failed. Skipping evaluation.\")\n",
    "    sys.stdout.flush()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}